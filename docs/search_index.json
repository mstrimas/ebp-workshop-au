[
["index.html", "eBird Best Practices Workshop Welcome", " eBird Best Practices Workshop Matthew Strimas-Mackey, Alison Johnston, Wesley M. Hochachka, Viviana Ruiz-Gutierrez, Orin J. Robinson, Eliot T. Miller, Tom Auer, Steve Kelling, Daniel Fink 2019-11-14 Welcome These lessons comprise a two part workshop on best practices for using eBird data. Part I of this workshop focuses on extracting and processing eBird data with R, while Part II covers using these data to model species distributions. The workshop as a whole is typically given over two days; however, it is designed to be modular and a half-day or one-day workshop can be given based on a subset of the material. In particular, Part I and II are largely independent of each other and can be taught individually. This workshop acts as a complement to the free online book eBird Best Practices as well as an associated paper on best practices for making reliable inferences from citizen science data. The contents of this workshop are addressed in greater detail in these resources. "],
["intro.html", "Lesson 1 Introduction 1.1 Data 1.2 Format 1.3 Setup 1.4 Tidyverse", " Lesson 1 Introduction The contents of this website comprise the notes for a workshop on best practices for using eBird data. Part I of this workshop will focus on extracting and processing eBird data, while Part II will cover using these data to model species distributions. The two parts are intentionally designed to be independent, which allows you to attend Part I, Part II, or both depending on your skills, background, and interests. 1.1 Data To follow along with the lessons in this workshop download the workshop data package. This package contains a variety of datasets for Parts I and II of this workshop. Instructions for how to use each dataset will be given in the relevant lesson. 1.2 Format This format of this workshop will be loosely based on Software Carpentry, the gold standard for workshops teaching scientific computing. As much as possible, the instructor will work through the lessons in real time, entering code live, while you code-along. Interspersed with the live coding will be exercises designed to give you a chance to practice on your own. This approach, known as participatory live coding, has been shown to be a much more effective means of learning to code than using slides or a pre-written script. Another Software Carpentry technique that we’ve adopted is the use of sticky notes. You should have two sticky notes at your desk, one blue and one yellow. Throughout the workshop, if you’ve completed an exercise or have passed a checkpoint, put the blue sticky note on your laptop to indicate that you’re done. Similarly, if you’re lost, stuck, or have a problem and need help, place the yellow sticky on your laptop. In addition to the instructor, we have several helpers roaming the room helping to troubleshoot problems. The sticky notes are the most effective way of flagging a helper down. Checkpoint Let’s stop here for a quick round on introductions, let us know who you are and how you hope to use eBird data! 1.3 Setup Before we dive into writing code, let’s take a few minutes to ensure our systems are properly set up with all the correct software and R packages. Devoting some time to this up front will reduce errors and make troubleshooting easier later in the workshop. If you followed the pre-workshop setup instructions, most of this setup should already be complete; however, we include it here to ensure everyone’s on the same page. Start by opening a browser window with four tabs pointing to the following websites: The shared Google Doc for this workshop (your instructor will provide a link). This will act as a collaborative notepad, which we can use to share code and links. Make sure you can edit the document. The eBird homepage The auk website. This auk R package is used to access eBird data and we’ll be using the website to view the documentation. The online lessons for this workshop. Checkpoint Are all tabs correctly opened? Can you edit the shared notepad? Next install or update RStudio, then open it. Look at the top line of the console, which gives your R version. If you have a version older than 3.5.0, you should consider updating R. Checkpoint Is your R version at least 3.5.0? Do you need help updating R? Create a new RStudio project called ebird-best-practices. Next, install all the packages required for this workshop and Part II, by running the following code: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;mstrimas/ebppackages&quot;) The auk package uses the unix command line tool AWK to access eBird data. AWK comes installed by default on Mac OS and Linux systems, but Windows users will need to install it. To do so, install the Cygwin software making sure to use the default install location. Checkpoint Is AWK installed? Run the following code to test that auk is installed correctly and AWK is working: library(auk) library(tidyverse) tf &lt;- tempfile() system.file(&quot;extdata/ebd-sample.txt&quot;, package = &quot;auk&quot;) %&gt;% auk_ebd() %&gt;% auk_species(species = c(&quot;Canada Jay&quot;, &quot;Blue Jay&quot;)) %&gt;% auk_country(country = c(&quot;US&quot;, &quot;Canada&quot;)) %&gt;% auk_bbox(bbox = c(-100, 37, -80, 52)) %&gt;% auk_date(date = c(&quot;2012-01-01&quot;, &quot;2012-12-31&quot;)) %&gt;% auk_time(start_time = c(&quot;06:00&quot;, &quot;09:00&quot;)) %&gt;% auk_duration(duration = c(0, 60)) %&gt;% auk_complete() %&gt;% auk_filter(tf) %&gt;% read_ebd() %&gt;% pull(common_name) %&gt;% message() unlink(tf) It should print Blue Jay. Checkpoint Did “Blue Jay” print without errors? If you’re running into any setup issues that can’t be resolved, use RStudio Cloud for this workshop instead. Your instructor will explain how to connect to RStudio Cloud. 1.4 Tidyverse Throughout this workshop, we’ll be using functions from the Tidyverse. This is an opinionated set of packages for working with data in R. Packages such as dplyr, ggplot2, and purrr are part of the Tidyverse. We’ll try to explain any functions as they come up; however, there’s one important operator from the Tidyverse that needs to be explained up front: the pipe operator %&gt;%. The pipe operator takes the expression to the left of it and “pipes” it into the first argument of the expression on the right. # without pipe mean(1:10) #&gt; [1] 5.5 # with pipe 1:10 %&gt;% mean() #&gt; [1] 5.5 The value of the pipe operator becomes clear when we have several operations in a row. Using the pipe makes code easier to read and reduces the need for intermediate variables. # without pipes set.seed(1) ran_norm &lt;- rnorm(10, sd = 5) ran_norm_pos &lt;- abs(ran_norm) ran_norm_sort &lt;- sort(ran_norm_pos) ran_norm_round &lt;- round(ran_norm_sort, digits = 1) ran_norm_round #&gt; [1] 0.9 1.5 1.6 2.4 2.9 3.1 3.7 4.1 4.2 8.0 # with pipes set.seed(1) rnorm(10, sd = 5) %&gt;% abs() %&gt;% sort() %&gt;% round(digits = 1) #&gt; [1] 0.9 1.5 1.6 2.4 2.9 3.1 3.7 4.1 4.2 8.0 For those that have never used the pipe, it probably looks strange, but if you stick with it, you’ll quickly come to appreciate it. Checkpoint Any questions about the pipe? Exercise Rewrite the following code using pipes: set.seed(1) round(log(runif(10, min = 0.5)), 1) Solution set.seed(1) runif(10, min = 0.5) %&gt;% log() %&gt;% round(digits = 1) #&gt; [1] -0.5 -0.4 -0.2 0.0 -0.5 -0.1 0.0 -0.2 -0.2 -0.6 "],
["ebird-intro.html", "Lesson 2 Introduction 2.1 The auk workflow", " Lesson 2 Introduction We’ll start with a short presentation giving an introduction to eBird and the motivation behind the auk R package. The presentation can be downloaded in PowerPoint or PDF format, or viewed on SpeakerDeck. 2.1 The auk workflow Extracting eBird data using auk requires proceeding through the following steps, each of which corresponds to a lesson in this document: Data access Filter Import Pre-process Produce presence-absence data After covering these fundamentals, we’ll proceed to some applications and more advanced topics, depending on time and interest: Spatiotemporal subsampling Mapping and sumarizing eBird data A variety of more advanced topics, including assigning land cover covariates to checklists, preparing data for occupancy modeling, and handling some of the file size issues that arise when working with eBird data "],
["access.html", "Lesson 3 Data Access", " Lesson 3 Data Access The complete eBird database (with the exception of sensitive species and observations that haven’t been approved) is provided via the eBird Basic Dataset (EBD), a large tab-separated text file released monthly. To access the EBD, you’ll need to create an eBird account and sign in to eBird. Once signed in, from the eBird homepage, click on the Science tab then scroll down to click on Using eBird for science in the right-hand menu. This page compiles links to a variety datasets, tools, and educational material for using eBird data to do science; it’s a great resource! Click on the “Download raw data here” link, then proceed to the eBird Basic Dataset page. If you haven’t already done so, you’ll need to submit a request to access the EBD. You can do this after the workshop if you haven’t already, we won’t need access today. From this page you can download the eBird Basic Dataset, a large tab-separated text file that contains (nearly) every eBird observation. In this file, each row corresponds to an observation of a species on a checklist. On this page, you can also download the Sampling Event Data. In this file, each row corresponds to a checklist rather than a species observation. We’ll see why this file is important later in the workshop. The EBD is huge, so don’t download these files now. Instead, we’ll be working with a small subset of the data today, which is available in the workshop data package discussed in the Introduction. This subset contains data from the Yucatan Peninsula (Guatemala, Belize, and five Mexican sates) from 2014-2015. The EBD and Sampling Event Data are in the edb/ subdirectory of the data package. Move these two text files to a sensible, central location on your computer (e.g. ~/data/ebird/). Note that when working with the full dataset, the downloaded files will be in .tar format, and you’ll need to unarchive them (this may require 7-Zip on Windows). The resulting directory will contain a file with extension .txt.gz, this file should be uncompressed to produce a text file, which you should transfer to the central data directory on your computer. The full EBD will be over 200 GB! If this is too large to fit on your computer, it can be stored on an external hard drive. We’ll also talk later in the workshop about some ways of avoiding downloading the EBD. Checkpoint Are the files downloaded and unarchived into a central location? Let’s take a look at this dataset. If we were working with the full EBD, we wouldn’t have enough memory to read in the whole file, but we can always read a small subset. Open a new R script (01_ebird-data.R) and read in the first few lines: library(auk) library(tidyverse) ebd_top &lt;- read_tsv(&quot;~/data/ebird/ebd_2014-2015_yucatan.txt&quot;, n_max = 5) View this data frame within RStudio by clicking on it in the Environment pane. Scroll over to the SAMPLING EVENT DATA column, which uniquely identifies checklists, and note the value: S17908640. With this ID we can access any non-private checklist via the website by appending it to https://ebird.org/view/checklist/. Look at the checklist online and compare it to the EBD. Notice two things that distinguish eBird data from other citizen science data: eBird collects data on the observation process, including the survey protocol used and effort information. This facilitates more robust analyses because we can account for variation in the observation process. Complete checklists enable non-detection to be inferred from the data. Without this, there’s no way to distinguish whether a species was not observed or just not reported. For these reasons, we refer to data from complete eBird checklists with effort information as semi-structured to distinguish from both most unstructured citizen science data and traditional structure scientific surveys. Exercise Take a few minutes to explore the EBD and compare it to the checklists online. Notice above that we had to reference the full path to the text files. In general, it’s best to avoid using absolute paths in R scripts because it makes them less portable–if you’re sharing the files with someone else, they’ll need to change the file paths to point to the location where they’ve stored the eBird data. The R package auk provides a solution to this, by allowing users to set an environment variable (EDB_PATH) that points to the directory containing the eBird data. To set this variable, use the function auk_set_ebd_path(). For example, if the EBD and Sampling Event Data files are in ~/data/ebird/, use: auk_set_ebd_path(&quot;~/data/ebird/&quot;) Tip You’ll need to restart R for these changes to take effect. The function auk_ebd() creates an R object referencing the EBD text file. Now that we’ve set up an EBD path, we can reference the EBD directly within auk_ebd() even though it’s not in our working directory. # the file isn&#39;t in out working directory file.exists(&quot;ebd_2014-2015_yucatan.txt&quot;) #&gt; [1] FALSE # yet auk can still find it auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) #&gt; Input #&gt; EBD: /Users/mes335/data/ebird/ebd_2014-2015_yucatan.txt #&gt; #&gt; Output #&gt; Filters not executed #&gt; #&gt; Filters #&gt; Species: all #&gt; Countries: all #&gt; States: all #&gt; BCRs: all #&gt; Bounding box: full extent #&gt; Date: all #&gt; Start time: all #&gt; Last edited date: all #&gt; Protocol: all #&gt; Project code: all #&gt; Duration: all #&gt; Distance travelled: all #&gt; Records with breeding codes only: no #&gt; Complete checklists only: no You should now see the EBD text file referenced as Input in this auk_ebd object. The remainder of the information printed will be the topic of the next section. Checkpoint Were you able to create an auk_ebd object referencing the EBD without specifying the full path? "],
["filter.html", "Lesson 4 Filtering 4.1 Defining filters 4.2 Execute filters", " Lesson 4 Filtering The EBD is huge, much too large to be read into R. So, if we want to work with these data, we first need to extract a small enough subset that it can be processed in R. This is the main purpose of the auk package: it uses the unix command line utility AWK to extract data from the EBD. There are three steps to this filtering process: Set up a reference to the EBD text file with auk_ebd(). Define a set of filters specifying the subset of data you want to extract. Compile those filters into an AWK script and run it to produce a text file with the desired subset of the data. Tip Filtering with auk can be fairly coarse, we just need to make the data small enough to read into R. Once the data are in R, they can further filtering can be used to refine the dataset. 4.1 Defining filters The types of filters that can be applied to the EBD fall into four categories: Species Region Season Protocol and effort Each specific filter is implemented by a different function in auk. Visit the documentation on filters on the auk website for a complete list. Each of these functions defines a filter on a column within the EBD. For example, auk_country() will define a filter allowing us to extract data from a subset of countries from the EBD. Tip Every filtering function in auk begins with auk_ for easy tab completion! To define a filter, start by creating an auk_ebd object, then pipe this into one of the filtering functions. library(auk) auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) %&gt;% auk_country(&quot;Guatemala&quot;) #&gt; Input #&gt; EBD: /Users/mes335/data/ebird/ebd_2014-2015_yucatan.txt #&gt; #&gt; Output #&gt; Filters not executed #&gt; #&gt; Filters #&gt; Species: all #&gt; Countries: GT #&gt; States: all #&gt; BCRs: all #&gt; Bounding box: full extent #&gt; Date: all #&gt; Start time: all #&gt; Last edited date: all #&gt; Protocol: all #&gt; Project code: all #&gt; Duration: all #&gt; Distance travelled: all #&gt; Records with breeding codes only: no #&gt; Complete checklists only: no Notice that when the auk_ebd object is printed, it tells us what filters have been defined. At this point, nothing has been done to the EBD, we’ve just defined the filter, we haven’t executed it yet. Tip Consult the Function Reference section of the auk website for a full list of available filters. In general, you should think about filtering on region, season, and species, so let’s build upon what we already have and add some more filters. For example, if we wanted all Resplendent Quetzal records from Guatemala in June 2015 we would use the following filters: auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(&quot;Resplendent Quetzal&quot;) %&gt;% auk_country(&quot;Guatemala&quot;) %&gt;% auk_date(c(&quot;2015-06-01&quot;, &quot;2015-06-30&quot;)) #&gt; Input #&gt; EBD: /Users/mes335/data/ebird/ebd_2014-2015_yucatan.txt #&gt; #&gt; Output #&gt; Filters not executed #&gt; #&gt; Filters #&gt; Species: Pharomachrus mocinno #&gt; Countries: GT #&gt; States: all #&gt; BCRs: all #&gt; Bounding box: full extent #&gt; Date: 2015-06-01 - 2015-06-30 #&gt; Start time: all #&gt; Last edited date: all #&gt; Protocol: all #&gt; Project code: all #&gt; Duration: all #&gt; Distance travelled: all #&gt; Records with breeding codes only: no #&gt; Complete checklists only: no Tip The filtering functions in auk check the arguments you provide and will throw an error if there’s something wrong. Filtering the EBD takes a long time, so it’s better to get an error now rather than realizing you made a mistake after waiting several hours for the extraction process to complete. auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) %&gt;% # typo in species name auk_species(&quot;Resplendant Quetzal&quot;) #&gt; Error in auk_species.auk_ebd(., &quot;Resplendant Quetzal&quot;): The following species were not found in the eBird taxonomy: #&gt; Resplendant Quetzal auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) %&gt;% # non-sequential dates auk_date(c(&quot;2015-06-01&quot;, &quot;2014-06-30&quot;)) #&gt; Error: date[1] not less than or equal to date[2] Exercise Define filters to extract Magnolia Warbler observations from Belize on checklists that started between 5 and 9 am and used either the “Traveling” or “Stationary” protocols. Consult the list of filters to see which ones you’ll need to use. Solution auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(&quot;Magnolia Warbler&quot;) %&gt;% auk_country(&quot;BZ&quot;) %&gt;% auk_protocol(c(&quot;Traveling&quot;, &quot;Stationary&quot;)) %&gt;% auk_time(c(&quot;5:00&quot;, &quot;9:00&quot;)) #&gt; Input #&gt; EBD: /Users/mes335/data/ebird/ebd_2014-2015_yucatan.txt #&gt; #&gt; Output #&gt; Filters not executed #&gt; #&gt; Filters #&gt; Species: Setophaga magnolia #&gt; Countries: BZ #&gt; States: all #&gt; BCRs: all #&gt; Bounding box: full extent #&gt; Date: all #&gt; Start time: 05:00-09:00 #&gt; Last edited date: all #&gt; Protocol: Traveling, Stationary #&gt; Project code: all #&gt; Duration: all #&gt; Distance travelled: all #&gt; Records with breeding codes only: no #&gt; Complete checklists only: no Tip In general, when using the effort filters like auk_time() or auk_distance(), it’s best to be a bit coarse. You can always refine the filters later once the data are in R, and starting with a coarse filter gives you some wiggle room if you later realize you want to make adjustments. Remember: the initial filtering with auk takes a long time, it’s best to limit the number of times you do this. When filtering by date, you may need to extract records from a given date range regardless of year. For this situation, the auk_date() function can accept wildcards for the year. For example, we can rewrite the above Resplendent Quetzal example to get observations from June of any year. auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(&quot;Resplendent Quetzal&quot;) %&gt;% auk_country(&quot;Guatemala&quot;) %&gt;% auk_date(c(&quot;*-06-01&quot;, &quot;*-06-30&quot;)) #&gt; Input #&gt; EBD: /Users/mes335/data/ebird/ebd_2014-2015_yucatan.txt #&gt; #&gt; Output #&gt; Filters not executed #&gt; #&gt; Filters #&gt; Species: Pharomachrus mocinno #&gt; Countries: GT #&gt; States: all #&gt; BCRs: all #&gt; Bounding box: full extent #&gt; Date: *-06-01 - *-06-30 #&gt; Start time: all #&gt; Last edited date: all #&gt; Protocol: all #&gt; Project code: all #&gt; Duration: all #&gt; Distance travelled: all #&gt; Records with breeding codes only: no #&gt; Complete checklists only: no 4.1.1 Complete checklists One of the most important filters is auk_complete(), which limits observations to those from complete checklists. As we’ve already seen, with complete checklists we can infer non-detections from the data. For most scientific applications, it’s critical that we have complete checklists, so we can generate presence-absence data. Exercise Define filters to extract Horned Guan and Highland Guan records from complete checklists in Chiapas, Mexico. Hint: look at the help for the auk_state() filter. Solution States are provided to auk_state() via 4-6 letter state codes. To find the state code, consult the ebird_states data frame or visit the Explore page on the eBird website and enter the state name under Explore Region. The state code will appear after region/ in the URL. For example, for Chiapas the URL is https://ebird.org/region/MX-CHP?yr=all and the state code is therefore MX-CHP. auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(c(&quot;Horned Guan&quot;, &quot;Highland Guan&quot;)) %&gt;% auk_state(&quot;MX-CHP&quot;) %&gt;% auk_complete() #&gt; Input #&gt; EBD: /Users/mes335/data/ebird/ebd_2014-2015_yucatan.txt #&gt; #&gt; Output #&gt; Filters not executed #&gt; #&gt; Filters #&gt; Species: Oreophasis derbianus, Penelopina nigra #&gt; Countries: all #&gt; States: MX-CHP #&gt; BCRs: all #&gt; Bounding box: full extent #&gt; Date: all #&gt; Start time: all #&gt; Last edited date: all #&gt; Protocol: all #&gt; Project code: all #&gt; Duration: all #&gt; Distance travelled: all #&gt; Records with breeding codes only: no #&gt; Complete checklists only: yes Checkpoint Are there any questions about defining filters on the EBD? 4.2 Execute filters Once you have an auk_ebd object with a set of filters defined, you can execute those filters with auk_filter(). This function compiles the filters into an AWK script, then runs that script to produce a text file with the defined subset of the EBD. The processing with AWK is done outside of R, line by line, only selecting rows that meet the criteria specified in the various auk filters. We’ll store the output file within the data/ subdirectory of the project directory. Note that filtering on the full EBD will take at least a couple hours, so be prepared to wait awhile. Let’s define filters to extract Yellow-rumped Warbler observations in Guatemala that appear on complete traveling or stationary checklists, then execute those filters. ebd_filtered &lt;- auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(&quot;Yellow-rumped Warbler&quot;) %&gt;% auk_country(&quot;GT&quot;) %&gt;% auk_protocol(c(&quot;Traveling&quot;, &quot;Stationary&quot;)) %&gt;% auk_complete() %&gt;% auk_filter(file = &quot;data/ebd_yerwar.txt&quot;) Take a look at this file and notice that we’ve drastically reduced the size. It can now be imported into R without any issues. Checkpoint Were you able to correctly extract the Yellow-rumped Warbler data? Any questions on filtering? "],
["import.html", "Lesson 5 Importing Data 5.1 Group checklists 5.2 Taxonomy", " Lesson 5 Importing Data In the previous lesson, we extracted a subset of the EBD containing Yellow-rumped Warbler observations from Guatemala. The output file created by auk_filter() is a tab-separated text file and could be read into R using read.delim() or readr::read_tsv(); however, auk has a function specifically for reading the EBD. read_ebd() does the following: Reads the data using data.table::fread(), which is much faster than read.delim(). Sets the correct data types for the columns. Cleans up the column names so they are all snake_case. Automatically performs some post processing steps, which will be covered later in this lesson. Let’s read in the data! library(auk) library(tidyverse) ebd &lt;- read_ebd(&quot;data/ebd_yerwar.txt&quot;, unique = FALSE, rollup = FALSE) glimpse(ebd) #&gt; Observations: 160 #&gt; Variables: 46 #&gt; $ global_unique_identifier &lt;chr&gt; &quot;URN:CornellLabOfOrnithology:EBIRD:… #&gt; $ last_edited_date &lt;chr&gt; &quot;2018-09-09 12:59:27&quot;, &quot;2017-08-29 … #&gt; $ taxonomic_order &lt;dbl&gt; 32863, 32859, 32858, 32858, 32858, … #&gt; $ category &lt;chr&gt; &quot;issf&quot;, &quot;issf&quot;, &quot;species&quot;, &quot;species… #&gt; $ common_name &lt;chr&gt; &quot;Yellow-rumped Warbler&quot;, &quot;Yellow-ru… #&gt; $ scientific_name &lt;chr&gt; &quot;Setophaga coronata&quot;, &quot;Setophaga co… #&gt; $ subspecies_common_name &lt;chr&gt; &quot;Yellow-rumped Warbler (Goldman&#39;s)&quot;… #&gt; $ subspecies_scientific_name &lt;chr&gt; &quot;Setophaga coronata goldmani&quot;, &quot;Set… #&gt; $ observation_count &lt;chr&gt; &quot;12&quot;, &quot;8&quot;, &quot;11&quot;, &quot;3&quot;, &quot;1&quot;, &quot;15&quot;, &quot;4… #&gt; $ breeding_bird_atlas_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ breeding_bird_atlas_category &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ age_sex &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ country &lt;chr&gt; &quot;Guatemala&quot;, &quot;Guatemala&quot;, &quot;Guatemal… #&gt; $ country_code &lt;chr&gt; &quot;GT&quot;, &quot;GT&quot;, &quot;GT&quot;, &quot;GT&quot;, &quot;GT&quot;, &quot;GT&quot;,… #&gt; $ state &lt;chr&gt; &quot;Huehuetenango&quot;, &quot;Petén&quot;, &quot;Huehuete… #&gt; $ state_code &lt;chr&gt; &quot;GT-HU&quot;, &quot;GT-PE&quot;, &quot;GT-HU&quot;, &quot;GT-JA&quot;,… #&gt; $ county &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ county_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ iba_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ bcr_code &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ usfws_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ atlas_block &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ locality &lt;chr&gt; &quot;Cerro de los Cuervos&quot;, &quot;Tikal Area… #&gt; $ locality_id &lt;chr&gt; &quot;L2713729&quot;, &quot;L4754006&quot;, &quot;L2713730&quot;,… #&gt; $ locality_type &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;H&quot;, &quot;P&quot;, &quot;P&quot;, … #&gt; $ latitude &lt;dbl&gt; 15.5, 17.2, 15.5, 14.7, 14.7, 15.5,… #&gt; $ longitude &lt;dbl&gt; -91.5, -89.6, -91.5, -90.0, -91.5, … #&gt; $ observation_date &lt;date&gt; 2014-03-07, 2014-01-19, 2014-03-05… #&gt; $ time_observations_started &lt;chr&gt; &quot;07:10:00&quot;, &quot;14:00:00&quot;, &quot;10:55:00&quot;,… #&gt; $ observer_id &lt;chr&gt; &quot;obsr200421&quot;, &quot;obsr837809&quot;, &quot;obsr20… #&gt; $ sampling_event_identifier &lt;chr&gt; &quot;S17445204&quot;, &quot;S36593691&quot;, &quot;S1744520… #&gt; $ protocol_type &lt;chr&gt; &quot;Traveling&quot;, &quot;Traveling&quot;, &quot;Travelin… #&gt; $ protocol_code &lt;chr&gt; &quot;P22&quot;, &quot;P22&quot;, &quot;P22&quot;, &quot;P22&quot;, &quot;P22&quot;, … #&gt; $ project_code &lt;chr&gt; &quot;EBIRD&quot;, &quot;EBIRD&quot;, &quot;EBIRD&quot;, &quot;EBIRD&quot;,… #&gt; $ duration_minutes &lt;int&gt; 170, 210, 55, 300, 120, 210, 180, 2… #&gt; $ effort_distance_km &lt;dbl&gt; 2.01, 1.61, 4.83, 5.00, 2.41, 2.01,… #&gt; $ effort_area_ha &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ number_observers &lt;int&gt; 2, 3, 3, 6, 1, 3, 3, 14, 4, 12, 14,… #&gt; $ all_species_reported &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… #&gt; $ group_identifier &lt;chr&gt; &quot;G828557&quot;, &quot;G2390002&quot;, &quot;G828560&quot;, N… #&gt; $ has_media &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, … #&gt; $ approved &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… #&gt; $ reviewed &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, FALSE, TR… #&gt; $ reason &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… #&gt; $ trip_comments &lt;chr&gt; NA, NA, &quot;Driving w/ stops&quot;, NA, &quot;Wa… #&gt; $ species_comments &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,… We’ll cover the use of unique = FALSE and rollup = FALSE next. For now, let’s just look at the data. Exercise Take a minute to explore these data using glimpse() and View(). Familiarize yourself with the columns. Be sure you can find the effort columns and the observation_count column. Checkpoint Do you have the data in a data frame? Does anyone have any questions about the data so far? 5.1 Group checklists eBird allows users to share checklists with other eBird users that they’re birding with. This results it multiple copies of some checklists in the database. Group checklists can be identified in the data because they have the group_identifier column populated. Let’s take a look at some these checklists. ebd %&gt;% filter(!is.na(group_identifier)) %&gt;% arrange(group_identifier) %&gt;% select(sampling_event_identifier, group_identifier) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; sampling_event_identifier group_identifier #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 S20741847 G1059449 #&gt; 2 S20713245 G1059449 #&gt; 3 S20713307 G1059450 #&gt; 4 S20741848 G1059450 #&gt; 5 S20925877 G1072982 #&gt; 6 S20929011 G1072982 We see that there are multiple checklists with the same group_identifier, implying that these checklists have been shared and are duplicates. Let’s look at one of these on the eBird website: https://ebird.org/view/checklist/S20741847 As it turns out, group checklists aren’t exact duplicates; once a checklist has been shared the individual checklists can diverge in terms of the species seen, the counts for each species, and even the protocol and effort. For an example, look at this checklist with six observers each of whom saw a different set of species. In most cases, you’ll only want to retain one of these checklists, but it’s not trivial to do so because the checklists are only partial duplicates. The function auk_unique() manages this for you. Specifically, for each species, it retains only the first observation of that species, which is typically the one submitted by the primary observer (i.e. the person who submit the checklist to eBird). Note that the resulting “checklist” will be a combination of all the species seen across all copies of the group checklist. keep_one &lt;- auk_unique(ebd) nrow(ebd) #&gt; [1] 160 nrow(keep_one) #&gt; [1] 104 When auk_unique() is run, a new field is created (checklist_id), which is populated with group_identifier for group checklists and sampling_event_identifier otherwise; this is now a unique identifier for checklists. In addition, the full set of observer and sampling event identifiers has been retained in a comma separated format. keep_one %&gt;% filter(!is.na(group_identifier)) %&gt;% select(checklist_id, sampling_event_identifier, group_identifier, observer_id) %&gt;% head() #&gt; # A tibble: 6 x 4 #&gt; checklist_id sampling_event_identif… group_identifier observer_id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 G828558 S17445097,S17445389 G828558 obsr200421,obsr325… #&gt; 2 G828561 S17445202,S17445397 G828561 obsr200421,obsr325… #&gt; 3 G828560 S17445203,S17445396 G828560 obsr200421,obsr325… #&gt; 4 G828557 S17445204,S17445204,S1… G828557 obsr200421,obsr200… #&gt; 5 G828555 S17445206,S17445382 G828555 obsr200421,obsr325… #&gt; 6 G828571 S17445344,S17445420 G828571 obsr200421,obsr325… By default, whenever you import data with read_ebd() it calls auk_unique() automatically; however, this behavior can be controlled with the unique argument. So, for example, the following will import data and remove duplicates. ebd &lt;- read_ebd(&quot;data/ebd_yerwar.txt&quot;, rollup = FALSE) Tip auk_unique() takes a long time to run on large datasets. Consider using read_ebd(unique = FALSE) when importing large text files to speed up the process. 5.2 Taxonomy eBird users can enter data for a wide range of taxa in addition to species. Observations can be reported at a level more granular than species (e.g. subspecies or recognizable forms) or at a higher level than species (e.g. spuhs, slashes, and hybrids). All the different taxa that can be reported are contained in the eBird taxonomy, which is updated every year in August. The eBird Science page has a subsection with details on the eBird taxonomy, and the taxonomy itself is available as a data frame in the auk package. glimpse(ebird_taxonomy) #&gt; Observations: 16,513 #&gt; Variables: 9 #&gt; $ species_code &lt;chr&gt; &quot;ostric2&quot;, &quot;ostric3&quot;, &quot;y00934&quot;, &quot;grerhe1&quot;, &quot;lesr… #&gt; $ scientific_name &lt;chr&gt; &quot;Struthio camelus&quot;, &quot;Struthio molybdophanes&quot;, &quot;S… #&gt; $ common_name &lt;chr&gt; &quot;Common Ostrich&quot;, &quot;Somali Ostrich&quot;, &quot;Common/Soma… #&gt; $ order &lt;chr&gt; &quot;Struthioniformes&quot;, &quot;Struthioniformes&quot;, &quot;Struthi… #&gt; $ family &lt;chr&gt; &quot;Struthionidae (Ostriches)&quot;, &quot;Struthionidae (Ost… #&gt; $ family_common &lt;chr&gt; &quot;Ostriches&quot;, &quot;Ostriches&quot;, &quot;Ostriches&quot;, &quot;Rheas&quot;, … #&gt; $ category &lt;chr&gt; &quot;species&quot;, &quot;species&quot;, &quot;slash&quot;, &quot;species&quot;, &quot;speci… #&gt; $ taxon_order &lt;dbl&gt; 1, 6, 7, 8, 14, 15, 18, 19, 20, 21, 26, 27, 30, … #&gt; $ report_as &lt;chr&gt; NA, NA, NA, NA, NA, &quot;lesrhe2&quot;, &quot;lesrhe2&quot;, NA, NA… # you can even report that you saw a generic bird! filter(ebird_taxonomy, common_name == &quot;bird sp.&quot;) #&gt; species_code scientific_name common_name order family family_common #&gt; 1 bird1 Aves sp. bird sp. &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; category taxon_order report_as #&gt; 1 spuh 34501 &lt;NA&gt; For taxa below the species level, the report_as field specifies the species that this taxa falls under. For example, Myrtle warbler rolls up to Yellow-rumped Warbler. # myrtle warbler filter(ebird_taxonomy, common_name == &quot;Yellow-rumped Warbler (Myrtle)&quot;) %&gt;% select(common_name, category, report_as) #&gt; common_name category report_as #&gt; 1 Yellow-rumped Warbler (Myrtle) issf yerwar # rolls up to yellow-rumped warbler filter(ebird_taxonomy, species_code == &quot;yerwar&quot;) %&gt;% select(common_name, category, report_as) #&gt; common_name category report_as #&gt; 1 Yellow-rumped Warbler species &lt;NA&gt; Exercise How many different subspecies of Barn Swallow does eBird recognize? Solution Start by finding the species code for Barn Swallow, then find records in ebird_taxonomy with this code in the report_as column. ebird_taxonomy %&gt;% filter(common_name == &quot;Barn Swallow&quot;) %&gt;% select(common_name, species_code) #&gt; common_name species_code #&gt; 1 Barn Swallow barswa ebird_taxonomy %&gt;% filter(report_as == &quot;barswa&quot;) %&gt;% select(common_name, category, report_as) #&gt; common_name category report_as #&gt; 1 Barn Swallow (White-bellied) issf barswa #&gt; 2 Barn Swallow (Egyptian) issf barswa #&gt; 3 Barn Swallow (Levant) issf barswa #&gt; 4 Barn Swallow (Tytler&#39;s) issf barswa #&gt; 5 Barn Swallow (Buff-bellied) issf barswa #&gt; 6 Barn Swallow (American) issf barswa eBird recognizes six subspecies. The EBD contains a subspecies column, which is populated when an observer has identified a bird below species level. In the EBD extract we’re working with, we have three different subspecies of Yellow-rumped Warbler: count(ebd, common_name, subspecies_common_name) #&gt; # A tibble: 4 x 3 #&gt; common_name subspecies_common_name n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Yellow-rumped Warbler Yellow-rumped Warbler (Audubon&#39;s) 27 #&gt; 2 Yellow-rumped Warbler Yellow-rumped Warbler (Goldman&#39;s) 4 #&gt; 3 Yellow-rumped Warbler Yellow-rumped Warbler (Myrtle) 16 #&gt; 4 Yellow-rumped Warbler &lt;NA&gt; 57 It’s even possible to have multiple subspecies of the same species on a single checklist. filter(ebd, checklist_id == &quot;S22725024&quot;) %&gt;% select(checklist_id, common_name, subspecies_common_name, observation_count) #&gt; # A tibble: 2 x 4 #&gt; checklist_id common_name subspecies_common_name observation_cou… #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 S22725024 Yellow-rumped Wa… Yellow-rumped Warbler (A… 1 #&gt; 2 S22725024 Yellow-rumped Wa… Yellow-rumped Warbler (M… 1 For most uses, you’ll want eBird data at the species level, which means dropping higher level taxa and rolling lower level taxa up to species level, making sure to sum the counts if multiple subspecies were present. The function auk_rollup() handles these taxonomic matters for you. no_subsp &lt;- auk_rollup(ebd) no_subsp %&gt;% filter(checklist_id == &quot;S22725024&quot;) %&gt;% select(checklist_id, common_name, observation_count) #&gt; # A tibble: 1 x 3 #&gt; checklist_id common_name observation_count #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 S22725024 Yellow-rumped Warbler 2 By default, when you import data with read_ebd() it calls auk_rollup() automatically; however, this behavior can be controlled with the rollup argument. So, for example, the following will import data and remove duplicates and report all records at species level. ebd &lt;- read_ebd(&quot;data/ebd_yerwar.txt&quot;) Checkpoint Any questions on data import, taxonomy, or group checklists? "],
["presabs.html", "Lesson 6 Presence-absence Data 6.1 Filtering 6.2 Zero-filling 6.3 Tidying up", " Lesson 6 Presence-absence Data Up to this point we’ve been working with presence-only data. The EBD, and eBird checklists in general, only explicitly record positive observations of species. However, if we limit ourselves to complete checklists, we can fill in the implied zero counts for any checklists on which a given species isn’t explicitly reported to generate presence-absence data. We refer to this process as zero-filling the eBird data. Zero-filling relies on the Sampling Event Data, which is a tab-seperated text file containing checklist-level information. This file contains the full population of checklists in the eBird database. If we apply exactly the same set of filters to both the EBD and the Sampling Event Data we can assume that any checklist with no observations for a given species in the EBD should get a zero-count record added to the dataset. So, producing presence-absence eBird data is a two-step process: Simultaneously filter the EBD and Sampling Event Data, making sure to only use complete checklists. Read both files into R and zero-fill the EBD using the full population of checklists from the Sampling Event Data. Tip When we say “presence-absence” what we really mean by “absence” is that the species was not detected, it’s entirely possible that the species was present, but the observer didn’t detect it. Checkpoint Are there any conceptual questions about the process of zero-filling? 6.1 Filtering Simultaneously filtering the EBD and Sampling Event Data is done in almost the exact same way as filtering the EBD alone. The only difference is that we provide both files to auk_ebd() and two corresponding output files to auk_filter(). For example, we can extract all American Flamingo observations from January in the Mexican state of Yucatán in preparation for zero-filling. library(auk) library(tidyverse) f_ebd &lt;- &quot;data/ebd_amefla.txt&quot; f_sed &lt;- &quot;data/sed_amefla.txt&quot; ebd_amefla &lt;- auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;, file_sampling = &quot;ebd_sampling_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(&quot;American Flamingo&quot;) %&gt;% auk_state(&quot;MX-YUC&quot;) %&gt;% auk_date(c(&quot;*-01-01&quot;, &quot;*-01-31&quot;)) %&gt;% auk_complete() %&gt;% auk_filter(f_ebd, file_sampling = f_sed) We now have two output files that have been extracted using the same set of filters, apart from the species filter, which only applies to the EBD. We can read these files into R individually: ebd_only &lt;- read_ebd(f_ebd) sed_only &lt;- read_sampling(f_sed) nrow(ebd_only) #&gt; [1] 47 nrow(sed_only) #&gt; [1] 291 So, we have 291 checklists in the Sampling Event Data and, of those, 47 have Flamingo observations on them. Checkpoint Were you able to filter and import the EBD and Sampling Event Data? Did you get the correct number of rows in both files? Exercise You’re studying Hooded Warblers wintering (November-February) in Belize. Extract eBird data in preparation for zero-filling, then read in the results and explore them. Hint: consult the Details section of the documentation for auk_date() to see how to filter a range of dates that wrap around the year end. Solution f_ebd_hw &lt;- &quot;data/ebd_hoowar.txt&quot; f_sed_hw &lt;- &quot;data/sed_hoowar.txt&quot; # filter ebd_hoowar &lt;- auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;, file_sampling = &quot;ebd_sampling_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(&quot;Hooded Warbler&quot;) %&gt;% auk_country(&quot;BZ&quot;) %&gt;% # when using wildcards, dates can wrap around the year end auk_date(c(&quot;*-11-01&quot;, &quot;*-02-29&quot;)) %&gt;% auk_complete() %&gt;% auk_filter(f_ebd_hw, file_sampling = f_sed_hw) # import the data ebd_only_hw &lt;- read_ebd(f_ebd_hw) sed_only_hw &lt;- read_sampling(f_sed_hw) 6.2 Zero-filling Now that we have these two datasets–containing checklist and species information, respectively–we can use the function auk_zerofill() to combine them to produce presence-absence data. This function also imports the data, and handles group checklists and taxonomic rollup automatically, we just have to pass it the paths to the two files. Let’s do this with the American Flamingo data. ebd_zf &lt;- auk_zerofill(f_ebd, sampling_events = f_sed) ebd_zf #&gt; Zero-filled EBD: 291 unique checklists, for 1 species. By default, auk_zerofill() returns the data as a list of two dataframes: sampling_events contains all the checklist and observations contains just the counts and presence-absence data for each species on each checklist. This compact format reduces the size of the data because checklist information isn’t replicated for every species observation. glimpse(ebd_zf$observations) #&gt; Observations: 291 #&gt; Variables: 4 #&gt; $ checklist_id &lt;chr&gt; &quot;G1089999&quot;, &quot;G1092350&quot;, &quot;G1095290&quot;, &quot;G1095467&quot;… #&gt; $ scientific_name &lt;chr&gt; &quot;Phoenicopterus ruber&quot;, &quot;Phoenicopterus ruber&quot;… #&gt; $ observation_count &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;3&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;… #&gt; $ species_observed &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE… glimpse(ebd_zf$sampling_events) #&gt; Observations: 291 #&gt; Variables: 31 #&gt; $ checklist_id &lt;chr&gt; &quot;S16201726&quot;, &quot;S21515362&quot;, &quot;S21431825&quot;,… #&gt; $ last_edited_date &lt;chr&gt; &quot;2014-01-03 11:28:47&quot;, &quot;2015-01-24 11:… #&gt; $ country &lt;chr&gt; &quot;Mexico&quot;, &quot;Mexico&quot;, &quot;Mexico&quot;, &quot;Mexico&quot;… #&gt; $ country_code &lt;chr&gt; &quot;MX&quot;, &quot;MX&quot;, &quot;MX&quot;, &quot;MX&quot;, &quot;MX&quot;, &quot;MX&quot;, &quot;M… #&gt; $ state &lt;chr&gt; &quot;Yucatán&quot;, &quot;Yucatán&quot;, &quot;Yucatán&quot;, &quot;Yuca… #&gt; $ state_code &lt;chr&gt; &quot;MX-YUC&quot;, &quot;MX-YUC&quot;, &quot;MX-YUC&quot;, &quot;MX-YUC&quot;… #&gt; $ county &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ county_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ iba_code &lt;chr&gt; NA, &quot;MX_183&quot;, &quot;MX_183&quot;, NA, NA, NA, NA… #&gt; $ bcr_code &lt;int&gt; 56, 55, 55, 55, 55, 55, 55, 55, 56, 56… #&gt; $ usfws_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ atlas_block &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ locality &lt;chr&gt; &quot;VALLADOLID&quot;, &quot;Celestun Casa Palmera&quot;,… #&gt; $ locality_id &lt;chr&gt; &quot;L2502912&quot;, &quot;L3305787&quot;, &quot;L3305787&quot;, &quot;L… #&gt; $ locality_type &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;P&quot;, &quot;P&quot;… #&gt; $ latitude &lt;dbl&gt; 20.7, 20.9, 20.9, 21.0, 20.7, 21.0, 21… #&gt; $ longitude &lt;dbl&gt; -88.2, -90.4, -90.4, -89.6, -89.7, -89… #&gt; $ observation_date &lt;date&gt; 2014-01-01, 2015-01-24, 2015-01-20, 2… #&gt; $ time_observations_started &lt;chr&gt; &quot;10:15:00&quot;, &quot;09:00:00&quot;, &quot;06:45:00&quot;, &quot;0… #&gt; $ observer_id &lt;chr&gt; &quot;obs439605&quot;, &quot;obs170749&quot;, &quot;obs170749&quot;,… #&gt; $ sampling_event_identifier &lt;chr&gt; &quot;S16201726&quot;, &quot;S21515362&quot;, &quot;S21431825&quot;,… #&gt; $ protocol_type &lt;chr&gt; &quot;Traveling&quot;, &quot;Stationary&quot;, &quot;Traveling&quot;… #&gt; $ protocol_code &lt;chr&gt; &quot;P22&quot;, &quot;P21&quot;, &quot;P22&quot;, &quot;P22&quot;, &quot;P21&quot;, &quot;P2… #&gt; $ project_code &lt;chr&gt; &quot;EBIRD_MEX&quot;, &quot;EBIRD&quot;, &quot;EBIRD&quot;, &quot;EBIRD&quot;… #&gt; $ duration_minutes &lt;int&gt; 90, 150, 120, 45, 30, 120, 5, 450, 105… #&gt; $ effort_distance_km &lt;dbl&gt; 1.609, NA, 0.322, 0.322, NA, 2.000, NA… #&gt; $ effort_area_ha &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ number_observers &lt;int&gt; 4, 1, 1, 2, 2, 1, 1, 3, 13, 1, 1, 5, 5… #&gt; $ all_species_reported &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR… #&gt; $ group_identifier &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ trip_comments &lt;chr&gt; &quot;RECORRIDO POR UNA HACIENDA.&quot;, &quot;from p… However, in this case object size isn’t an issue, and it’s easier to work with a single dataframe, so we can collapse the data with collapse_zerofill(). ebd_zf_df &lt;- collapse_zerofill(ebd_zf) glimpse(ebd_zf_df) #&gt; Observations: 291 #&gt; Variables: 34 #&gt; $ checklist_id &lt;chr&gt; &quot;S16201726&quot;, &quot;S21515362&quot;, &quot;S21431825&quot;,… #&gt; $ last_edited_date &lt;chr&gt; &quot;2014-01-03 11:28:47&quot;, &quot;2015-01-24 11:… #&gt; $ country &lt;chr&gt; &quot;Mexico&quot;, &quot;Mexico&quot;, &quot;Mexico&quot;, &quot;Mexico&quot;… #&gt; $ country_code &lt;chr&gt; &quot;MX&quot;, &quot;MX&quot;, &quot;MX&quot;, &quot;MX&quot;, &quot;MX&quot;, &quot;MX&quot;, &quot;M… #&gt; $ state &lt;chr&gt; &quot;Yucatán&quot;, &quot;Yucatán&quot;, &quot;Yucatán&quot;, &quot;Yuca… #&gt; $ state_code &lt;chr&gt; &quot;MX-YUC&quot;, &quot;MX-YUC&quot;, &quot;MX-YUC&quot;, &quot;MX-YUC&quot;… #&gt; $ county &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ county_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ iba_code &lt;chr&gt; NA, &quot;MX_183&quot;, &quot;MX_183&quot;, NA, NA, NA, NA… #&gt; $ bcr_code &lt;int&gt; 56, 55, 55, 55, 55, 55, 55, 55, 56, 56… #&gt; $ usfws_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ atlas_block &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ locality &lt;chr&gt; &quot;VALLADOLID&quot;, &quot;Celestun Casa Palmera&quot;,… #&gt; $ locality_id &lt;chr&gt; &quot;L2502912&quot;, &quot;L3305787&quot;, &quot;L3305787&quot;, &quot;L… #&gt; $ locality_type &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;H&quot;, &quot;H&quot;, &quot;H&quot;, &quot;P&quot;, &quot;P&quot;… #&gt; $ latitude &lt;dbl&gt; 20.7, 20.9, 20.9, 21.0, 20.7, 21.0, 21… #&gt; $ longitude &lt;dbl&gt; -88.2, -90.4, -90.4, -89.6, -89.7, -89… #&gt; $ observation_date &lt;date&gt; 2014-01-01, 2015-01-24, 2015-01-20, 2… #&gt; $ time_observations_started &lt;chr&gt; &quot;10:15:00&quot;, &quot;09:00:00&quot;, &quot;06:45:00&quot;, &quot;0… #&gt; $ observer_id &lt;chr&gt; &quot;obs439605&quot;, &quot;obs170749&quot;, &quot;obs170749&quot;,… #&gt; $ sampling_event_identifier &lt;chr&gt; &quot;S16201726&quot;, &quot;S21515362&quot;, &quot;S21431825&quot;,… #&gt; $ protocol_type &lt;chr&gt; &quot;Traveling&quot;, &quot;Stationary&quot;, &quot;Traveling&quot;… #&gt; $ protocol_code &lt;chr&gt; &quot;P22&quot;, &quot;P21&quot;, &quot;P22&quot;, &quot;P22&quot;, &quot;P21&quot;, &quot;P2… #&gt; $ project_code &lt;chr&gt; &quot;EBIRD_MEX&quot;, &quot;EBIRD&quot;, &quot;EBIRD&quot;, &quot;EBIRD&quot;… #&gt; $ duration_minutes &lt;int&gt; 90, 150, 120, 45, 30, 120, 5, 450, 105… #&gt; $ effort_distance_km &lt;dbl&gt; 1.609, NA, 0.322, 0.322, NA, 2.000, NA… #&gt; $ effort_area_ha &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ number_observers &lt;int&gt; 4, 1, 1, 2, 2, 1, 1, 3, 13, 1, 1, 5, 5… #&gt; $ all_species_reported &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR… #&gt; $ group_identifier &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… #&gt; $ trip_comments &lt;chr&gt; &quot;RECORRIDO POR UNA HACIENDA.&quot;, &quot;from p… #&gt; $ scientific_name &lt;chr&gt; &quot;Phoenicopterus ruber&quot;, &quot;Phoenicopteru… #&gt; $ observation_count &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;15… #&gt; $ species_observed &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FAL… Notice that in addition to the observation_count column, we now have a binary species_observered column specifying whether or not the species was observered on this checklist. You can also automatically collapse the data by using the collapse = TRUE argument to auk_zerofill(). Exercise Zero-fill and collapse the Hooded Warbler data you extracted in the previous exercise. What proportion of checklists detected this species? Solution ebd_zf_hw &lt;- auk_zerofill(f_ebd_hw, f_sed_hw, collapse = TRUE) # proportion of checklists mean(ebd_zf_hw$species_observed) #&gt; [1] 0.268 Tip Whenever you’re zero-filling data it’s critical that you think about region and season (i.e. where and when) in addition to just the species. If you don’t do that, you’ll zero-fill the entire global EBD and your computer will explode! For example, consider a highly localized species like the Cozumel Vireo, endemic to the small island of Cozumel off the coast of Mexico. Let’s try just filtering on species. ebd_cozvir &lt;- auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;, file_sampling = &quot;ebd_sampling_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(&quot;Cozumel Vireo&quot;) %&gt;% auk_complete() %&gt;% auk_filter(&quot;data/ebd_cozvir.txt&quot;, &quot;data/sed_cozvir.txt&quot;) %&gt;% auk_zerofill(collapse = TRUE) table(ebd_cozvir$species_observed) #&gt; #&gt; FALSE TRUE #&gt; 22250 107 What we have here is the entire EBD (22 thousand checklists in the example dataset, and 40 million in the full EBD!) for a species that only occurs on one small island. Do we really care that a checklist in Anchorage, Alaska doesn’t have Cozumel Vireo? In this situation, you would be better to identify the boundaries of the island and use auk_bbox() to spatially subset the data. ebd_cozvir &lt;- auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;, file_sampling = &quot;ebd_sampling_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(&quot;Cozumel Vireo&quot;) %&gt;% # lng_min, lat_min, lng_max, lat_max auk_bbox(c(-87.1, 20.2, -86.7, 20.6)) %&gt;% auk_complete() %&gt;% auk_filter(&quot;data/ebd_cozvir.txt&quot;, &quot;data/sed_cozvir.txt&quot;, overwrite = TRUE) %&gt;% auk_zerofill(collapse = TRUE) table(ebd_cozvir$species_observed) #&gt; #&gt; FALSE TRUE #&gt; 433 107 We have the same number of positive observations, but have now drastically reduced the number of checklists that didn’t detect Cozumel Vireo observations. 6.3 Tidying up We now have a zero-filled presence-absence dataset with duplicate group checklists removed and all observations at the species level. There are couple remaining steps that we typically run to clean up the data. First, you may have noticed some cases where observation_count is &quot;X&quot; in the data. This is what eBirders enter for the count to indicate that they didn’t count the number of individuals for a given species. arrange(ebd_zf_df, desc(observation_count)) %&gt;% select(checklist_id, observation_count) %&gt;% head(10) #&gt; # A tibble: 10 x 2 #&gt; checklist_id observation_count #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 S26749776 X #&gt; 2 S16248765 X #&gt; 3 S16540721 X #&gt; 4 S16490447 X #&gt; 5 S16520714 X #&gt; 6 S16382176 X #&gt; # … with 4 more rows It’s more appropriate to have the count as NA rather than &quot;X&quot; in this scenario. This will also allow us to convert the count column to integer rather than character. At this point, we’ll also assign an explicit distance of 0 to stationary checklists. zf_count &lt;- ebd_zf_df %&gt;% mutate(observation_count = if_else(observation_count == &quot;X&quot;, NA_character_, observation_count), observation_count = as.integer(observation_count), effort_distance_km = if_else(protocol_type == &quot;Stationary&quot;, 0, effort_distance_km)) Finally, depending on your application, you’ll likely want to do some further filtering of the data. For many uses, it’s a good idea to reduce the variation in detectability between checklists by imposing some constraints on the effort variables. You can think of this as partially standardizing the observation process in a post hoc fashion. For example, in part II of this workshop, we’ll restrict observations to those from checklists less than 5 hours long and 5 km in length, and with 10 or fewer observers. zf_effort &lt;- zf_count %&gt;% filter(duration_minutes &lt;= 60 * 5, effort_distance_km &lt;= 5, number_observers &lt;= 10) table(zf_count$species_observed) #&gt; #&gt; FALSE TRUE #&gt; 244 47 table(zf_effort$species_observed) #&gt; #&gt; FALSE TRUE #&gt; 217 25 We’ve reduced the amount of data, but also decreased the variability in effort, which will lead to better model performance if we use these data to model species distributions. At this point, we can save the resulting processed eBird data, so that we can use it later in our analysis workflow. write_csv(zf_effort, &quot;data/ebird_amefla_zf.csv&quot;) "],
["subsampling.html", "Lesson 7 Spatiotemporal Subsampling 7.1 A toy example 7.2 Subsampling eBird data", " Lesson 7 Spatiotemporal Subsampling Despite the strengths of eBird data, species observations collected through citizen science projects present a number of challenges that are not found in conventional scientific data. In this chapter, we’ll discuss three of these challenges: spatial bias, temporal bias, and class imbalance. Spatial and temporal bias refers to the tendency of eBird checklists to be distributed non-randomly in space and time, while class imbalance refers to fact that there will be many more non-detections than detections for most species. All three can impact our ability to make reliable inferences from eBird data. Exercise Think of some examples of birder behavior that can lead to spatial and temporal bias. Solution Spatial bias: most eBirders sample near their homes, in easily accessible areas such as roadsides, or in areas and habitats of known high biodiversity. Temporal bias: eBirders preferentially sample when they are available, such as weekends, and at times of year when they expect to observe more birds, such as spring migration in North America. Fortunately, all three of these challenges can largely be addressed by spatiotemporal subsampling of the eBird data prior to modeling. In particular, this consists of dividing space and time up into a regular grid (e.g. 5 km x 5 km x 1 week), and only selecting a subset of checklists from each grid cell. To deal with class imbalance, we can subsample detections and non-detections separately to ensure we don’t lose too many detections. For the spatial part of the subsampling we’ll use the package dggridR to generate a regular hexagonal grid and assign points to the cells of this grid. Hexagonal grids are preferable to square grids because they exhibit significantly less spatial distortion. 7.1 A toy example To illustrate how spatial sampling on a hexagonal grid works, let’s start with a simpe toy example. We’ll generate 500 randomly placed points, construct a hexagonal grid with 5 km spacing, assign each point to a grid cell, then select a single point within each cell. library(auk) library(sf) library(dggridR) library(lubridate) library(tidyverse) # bounding box to generate points from bb &lt;- st_bbox(c(xmin = -0.1, xmax = 0.1, ymin = -0.1, ymax = 0.1), crs = 4326) %&gt;% st_as_sfc() %&gt;% st_sf() # random points pts &lt;- st_sample(bb, 500) %&gt;% st_sf(as.data.frame(st_coordinates(.)), geometry = .) %&gt;% rename(lat = Y, lon = X) # contruct a hexagonal grid with ~ 5 km between cells dggs &lt;- dgconstruct(spacing = 5) #&gt; Resolution: 13, Area (km^2): 31.9926151554038, Spacing (km): 5.58632116604266, CLS (km): 6.38233997895802 # for each point, get the grid cell pts$cell &lt;- dgGEO_to_SEQNUM(dggs, pts$lon, pts$lat)$seqnum # sample one checklist per grid cell pts_ss &lt;- pts %&gt;% group_by(cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() # generate polygons for the grid cells hexagons &lt;- dgcellstogrid(dggs, unique(pts$cell), frame = FALSE) %&gt;% st_as_sf() ggplot() + geom_sf(data = hexagons) + geom_sf(data = pts, size = 0.5) + geom_sf(data = pts_ss, col = &quot;red&quot;) + theme_bw() In the above plot, black dots represent the original set of 500 randomly placed points, while red dots represent the subsampled data, one point per hexagonal cell. 7.2 Subsampling eBird data Now let’s apply this same approach to the zero-filled American Flamingo data we produced in the previous lesson; however, now we’ll temporally sample as well, at a resolution of one week, and sample presences and absences separately. We start by reading in the eBird data and assigning each checklist to a hexagonal grid cell and a week. # generate hexagonal grid with ~ 5 km betweeen cells dggs &lt;- dgconstruct(spacing = 5) #&gt; Resolution: 13, Area (km^2): 31.9926151554038, Spacing (km): 5.58632116604266, CLS (km): 6.38233997895802 # read in data ebird &lt;- read_csv(&quot;data/ebird_amefla_zf.csv&quot;) %&gt;% # get hexagonal cell id and week number for each checklist mutate(cell = dgGEO_to_SEQNUM(dggs, longitude, latitude)$seqnum, year = year(observation_date), week = week(observation_date)) Now we sample a single checklist from each grid cell for each week, using group_by() to sample presences and absences separetely. ebird_ss &lt;- ebird %&gt;% group_by(species_observed, year, week, cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() Exercise How did the spatiotemporal subsampling affect the overall sample size as well as the prevalence of detections? Solution # original data nrow(ebird) #&gt; [1] 242 count(ebird, species_observed) %&gt;% mutate(percent = n / sum(n)) #&gt; # A tibble: 2 x 3 #&gt; species_observed n percent #&gt; &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 FALSE 217 0.897 #&gt; 2 TRUE 25 0.103 # after sampling nrow(ebird_ss) #&gt; [1] 147 count(ebird_ss, species_observed) %&gt;% mutate(percent = n / sum(n)) #&gt; # A tibble: 2 x 3 #&gt; species_observed n percent #&gt; &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 FALSE 126 0.857 #&gt; 2 TRUE 21 0.143 So, the subsampling decreased the overall number of checklists from 242 to 147, but increased the prevalence of detections from 10.3% to 14.3%. "],
["applications.html", "Lesson 8 Applications 8.1 Frequency trajectories 8.2 Maps", " Lesson 8 Applications Now that we know how to extract and zero-fill data from eBird, let’s do something with these data! We’ll start by summarizing the data to show the trajectory of observation frequency over the course of the year. Next, we’ll make a basic presence-absence map of eBird observations. More advanced topics are covered in part II of this workshop, which focuses on using eBird data to model species distributions. We’ll start a new script for this lesson: 02_applications.R. 8.1 Frequency trajectories The genus Cardellina contains five New World warbler species, including some of the most spectacular birds in North America: Canada Warbler, Red Warbler, and Pink-headed Warbler. Let’s extract and zero-fill data for three of the species in this genus within Guatemala. library(auk) library(sf) library(rnaturalearth) library(lubridate) library(tidyverse) f_ebd &lt;- &quot;data/ebd_cardellina.txt&quot; f_sed &lt;- &quot;data/sed_cardellina.txt&quot; # extract filters &lt;- auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;, file_sampling = &quot;ebd_sampling_2014-2015_yucatan.txt&quot;) %&gt;% auk_species(c(&quot;Canada Warbler&quot;, &quot;Wilson&#39;s Warbler&quot;, &quot;Pink-headed Warbler&quot;)) %&gt;% auk_country(&quot;Guatemala&quot;) %&gt;% auk_complete() %&gt;% auk_filter(f_ebd, f_sed) # zero-fill cardellina_zf &lt;- auk_zerofill(f_ebd, f_sed, collapse = TRUE) count(cardellina_zf, scientific_name, species_observed) #&gt; # A tibble: 6 x 3 #&gt; scientific_name species_observed n #&gt; &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; #&gt; 1 Cardellina canadensis FALSE 6454 #&gt; 2 Cardellina canadensis TRUE 54 #&gt; 3 Cardellina pusilla FALSE 4795 #&gt; 4 Cardellina pusilla TRUE 1713 #&gt; 5 Cardellina versicolor FALSE 6322 #&gt; 6 Cardellina versicolor TRUE 186 Next, let’s summarize these data, calculating the frequency of observation on eBird checklists by month. cardellina_freq &lt;- cardellina_zf %&gt;% mutate(month = month(observation_date)) %&gt;% group_by(scientific_name, month) %&gt;% summarize(obs_freq = mean(species_observed)) %&gt;% ungroup() In preparation for plotting, we can add the common names of the species by joining in the ebird_taxonomy data frame, which is included with auk. In addition, we’ll convert the integer month numbers to dates–using the midpoint of each month–to aid axis labeling. cardellina_comm &lt;- cardellina_freq %&gt;% inner_join(ebird_taxonomy, by = &quot;scientific_name&quot;) %&gt;% select(common_name, month, obs_freq) %&gt;% mutate(month_midpoint = ymd(str_glue(&quot;2019-{month}-15&quot;))) Finally, let’s make a frequency trajectory for these four species. ggplot(cardellina_comm) + aes(x = month_midpoint, y = obs_freq, color = common_name) + geom_point() + geom_line() + scale_x_date(date_breaks = &quot;2 months&quot;, date_labels = &quot;%b&quot;) + scale_color_brewer(palette = &quot;Set1&quot;) + labs(x = &quot;Month&quot;, y = &quot;Observation Frequency&quot;, color = NULL) + theme(legend.position = &quot;bottom&quot;) We have several different patterns going on here: Pink-headed Warblers is a resident species, present year-round at fairly low abundance Wilson’s Warbler spends the winter in Guatemala and is common during that period Canada Warbler appears to only pass through Guatemala during migration 8.2 Maps Continuing with the same dataset, let’s make a presence-absence map for Pink-headed Warbler. We’ll start by filtering the zero-filled data to only Pink-headed Warbler observations and converting these points to a spatial format using the sf package. pihwar &lt;- cardellina_zf %&gt;% filter(scientific_name == &quot;Cardellina versicolor&quot;) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) Next, we’ll use rnaturalearth to get some contextual GIS data to use in our maps. This is an amazing source for free GIS data for making maps. ne_country &lt;- ne_countries(continent = &quot;North America&quot;, returnclass = &quot;sf&quot;) %&gt;% st_geometry() ne_gt &lt;- ne_countries(country = &quot;Guatemala&quot;, returnclass = &quot;sf&quot;) %&gt;% st_geometry() # restrict to points falling within Guatemala, removes those over water pihwar &lt;- pihwar[ne_gt, ] Finally, we’ll make a presence-absence map, building it up in layers. There are lots excellent tools for mapping in R; however, here we’ll use the basic plot() function from the sf package. Other good options include ggplot2 and tmap. par(mar = c(0.25, 0.25, 0.25, 0.25)) # start by defining the bounds of the map with an empty plot plot(ne_gt, col = NA, border = NA) # borders plot(ne_country, col = &quot;grey80&quot;, border = &quot;white&quot;, add = TRUE) plot(ne_gt, col = &quot;grey70&quot;, border = &quot;white&quot;, add = TRUE) # not observed pihwar_abs &lt;- filter(pihwar, !species_observed) %&gt;% st_geometry() plot(pihwar_abs, col = alpha(&quot;grey20&quot;, 0.3), pch = 19, cex = 0.25, add = TRUE) # present pihwar_pres &lt;- filter(pihwar, species_observed) %&gt;% st_geometry() plot(pihwar_pres, col = alpha(&quot;orange&quot;, 1), pch = 19, cex = 0.5, add = TRUE) title(&quot;Pink-headed Warbler eBird Observations&quot;, line = -1) legend(&quot;bottomright&quot;, col = c(&quot;grey20&quot;, &quot;orange&quot;), legend = c(&quot;Not reported&quot;, &quot;Present&quot;), pch = 19) box() Based on this map, we see that Pink-headed Warbler is restricted to the southwestern highlands of Guatemala. Exercise Make a similar map of Wilson’s Warbler (Cardellina pusilla) observations. Solution # prepare data wilwar &lt;- cardellina_zf %&gt;% filter(scientific_name == &quot;Cardellina pusilla&quot;) %&gt;% mutate(pres_abs = if_else(species_observed, &quot;Present&quot;, &quot;Not detected&quot;)) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) wilwar &lt;- wilwar[ne_gt, ] # make map par(mar = c(0.25, 0.25, 0.25, 0.25)) plot(ne_gt, col = NA, border = NA) plot(ne_country, col = &quot;grey80&quot;, border = &quot;white&quot;, add = TRUE) plot(ne_gt, col = &quot;grey70&quot;, border = &quot;white&quot;, add = TRUE) # not observed wilwar_abs &lt;- filter(wilwar, !species_observed) %&gt;% st_geometry() plot(wilwar_abs, col = alpha(&quot;grey20&quot;, 0.3), pch = 19, cex = 0.25, add = TRUE) # present wilwar_pres &lt;- filter(wilwar, species_observed) %&gt;% st_geometry() plot(wilwar_pres, col = alpha(&quot;orange&quot;, 1), pch = 19, cex = 0.5, add = TRUE) title(&quot;Wilson&#39;s Warbler eBird Observations&quot;, line = -1) legend(&quot;bottomright&quot;, col = c(&quot;grey20&quot;, &quot;orange&quot;), legend = c(&quot;Not reported&quot;, &quot;Present&quot;), pch = 19) box() "],
["advanced.html", "Lesson 9 Advanced Topics 9.1 Preparing for occupancy modeling 9.2 Land cover covariates 9.3 Custom downloads 9.4 Reducing file size", " Lesson 9 Advanced Topics 9.1 Preparing for occupancy modeling One common use of eBird data is for occupancy modeling with the R package unmarked. We’ll cover occupancy modeling in detail in Part II of this workshop; however, unmarked requires data to be formatted in a very particular way and auk contains functions to do this, which we’ll cover in this lesson. We’ll continue with the American Flamingo data generated in Lesson 6. First, we need to extract a subset of observations that are suitable for occupancy modeling. In particular, occupancy models typically require data from repeated visits to a single site during a time frame over which the population can be considered closed. The auk function filter_repeat_visits() is designed to extract subsets of eBird data that meet these criteria. For example, let’s define the period of closure as the entire month of January in each year and a repeat visit as the same observer revisiting a site with same latitude and longitude at least twice. The relevant parameters in filter_repeat_visits are: min_obs and max_obs: the minimum and maximum number of repeat visits to a given site. Occupancy modeling requires at least two visits to each site. date_var: the column name of the date variable used to define the period of closure. annual_closure: define the period of closure as the entire year. This works here because we’ve already subset the data to only keep observations from January, which results in the period of closure being the month of January in given year. The n_days argument can be used to define the period of closure more flexibly. site_vars: a character vector of names of columns that define a site. This is typically the combination of variables defining the location (latitude and longitude) and observer (observer_id). library(auk) library(lubridate) library(tidyverse) # read in the american flamingo data ebd_zf &lt;- read_csv(&quot;data/ebird_amefla_zf.csv&quot;) # extract data suitable for occupancy modeling visits &lt;- filter_repeat_visits(ebd_zf, min_obs = 2, max_obs = 10, date_var = &quot;observation_date&quot;, annual_closure = TRUE, site_vars = c(&quot;latitude&quot;, &quot;longitude&quot;, &quot;observer_id&quot;)) # entire data set nrow(ebd_zf) #&gt; [1] 242 # reduced data set nrow(visits) #&gt; [1] 122 # how many individual sites there are n_distinct(visits$site) #&gt; [1] 40 Three new columns are added to the dataset when using the function filter_repeat_visits(): site is a unique site ID, closure_id identifies the primary period of closure (in this example the year), and n_observations is the number of visits to each site. visits %&gt;% select(site, closure_id, n_observations) %&gt;% head() #&gt; # A tibble: 6 x 3 #&gt; site closure_id n_observations #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 20.0893017_-89.551304_obs174363_2014 2014 2 #&gt; 2 20.0893017_-89.551304_obs174363_2014 2014 2 #&gt; 3 20.359625_-89.7705531_obs282508_2015 2015 3 #&gt; 4 20.359625_-89.7705531_obs282508_2015 2015 3 #&gt; 5 20.359625_-89.7705531_obs282508_2015 2015 3 #&gt; 6 20.5685104_-88.5133266_obs431367_2014 2014 4 Now that we have data suitable for occupancy modeling, we need to reformat the data to be accepted by unmarked. The documentation for the unmarked function formatWide() outlines the details of this format. In the EBD, each row is a checklist; however, unmarked requires each row to be a site with the first column specifying the site ID and subsequent columns specifying whether the species was observed on each of the visits to that site. The next group of columns contains site-level covariates, those that vary between sites but are constant across visits to the same site, such as latitude, longitude, and any habitat covariates we might have. Finally, the observation-level covariates, such as distance and duration, each get a set of columns corresponding to the the presence-absence columns. Here’s a simple example with some made up data to illustrate the format: site_id y.1 y.2 y.3 latitude longitude forest_cover distance.1 distance.2 distance.3 time.1 time.2 time.3 site1 TRUE FALSE TRUE 20.2 182 0.12 14.51 10.01 12.41 33.7 43.5 20.7 site2 FALSE TRUE 20.6 183 0.45 9.84 11.90 26.4 23.8 site3 TRUE FALSE FALSE 19.9 182 0.98 8.95 12.63 9.78 23.4 30.1 13.3 site4 TRUE FALSE 21.0 183 0.23 10.26 6.00 31.9 26.9 site5 FALSE FALSE FALSE 29.8 183 0.43 11.34 7.58 16.88 24.8 25.0 23.6 The auk function format_unmarked_occu() takes care of the reformatting for you. In this function, site_covs are the names of the site-level covariates and obs_covs are the names of the observation-level covariates. visits_um &lt;- format_unmarked_occu(visits, site_covs = c(&quot;latitude&quot;, &quot;longitude&quot;), obs_covs = c(&quot;time_observations_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;)) Exercise Explore both the visits_um and visits data frames. They contain the same data in different formats. Try to understand how one data frame was transformed into the other. 9.2 Land cover covariates To use eBird data for modeling species distributions you’ll typically want to attach habitat data to the checklists to use as model covariates. There are a wide variety of freely available, satellite-derived land use and land cover products available for use as covariates. The specific set of covariates you use, and the covariates that you derived them from, will depend on the specifics of your study: the focal species and region, and question you’re asking. However, we recommend the MODIS MCD12Q1 land cover dataset as a good general use source of land cover covariates. This product is global in extent, available annually since 2001, and provided in a raster format at 500 m resolution. The free eBird Best Practices book associated with this workshop covers in detail how to download land cover data and attach it to eBird checklists. However, we provide an overview of the process here. Download: download land cover data (e.g. MODIS MCD12Q1) for your region of interest, either manually or using the R package MODIS. These data will often be downloaded in the form of multiple tiles that need to be combined together then converted to GeoTIFF format. This is done automatically if you’re using the MOIDS package, but it can be done manually using the command line tool GDAL. Summarize: birds use and respond to their environment at a scale that is typically larger than the resolution of the satellite data. Furthermore, there is uncertainty in the location of eBird checklists, particularly traveling checklists. So, you should usually summarize the data within a neighborhood of each checklist. This can be done most simply by calculating the percent of each land cover class within a neighborhood of each point. Prediction surface: once you’ve fit your models using the land cover covariates you generated in the previous steps, you’ll usually want to make predictions over the landscape and produce maps. This requires defining a regular grid of points at which you calculate the same land cover covariates as you did in the previous step. To see the code for each of these steps consult the eBird Best Practices book. 9.3 Custom downloads The full EBD is massive (over 200 GB) and takes a long time to process with auk (typically several hours). In some cases, there’s a way around these issues. On the EBD download page, there’s a Custom Download form that allows you to request a subset of the EBD for a given species, within a region, and for a range of dates. After submitting a request, it will be processed on the eBird servers and an email will be sent to you with instructions for downloading the EBD extract. Recall from the lesson on zero-filling, that we extracted American Flamingo records from Mexico’s Yucatán state in January, and produced presence-absence data from this extract. Let’s try to do this with the Custom Download form. After the request is submitted an email will arrive with instrustions for downloading the following file. Download and unarchive this file, placing the text file in the data/ subdirectory of your project. It will quickly become clear that there are two challenges with this approach. First, the set of filters available in the Custom Download form is limited. For example, there’s no option to only extract observations from complete checklists or any way to get observations from a given month from any year. To address this, we can apply the additional filters after we’ve imported the data in R. Specifically, we’ll only keep observations from complete checklists in January ebd &lt;- read_ebd(&quot;data/ebd_MX-YUC_grefla2_201401_201512_relSep-2019.txt&quot;) %&gt;% filter(month(observation_date) == 1, all_species_reported) The second challenge is that the Custom Download form only provides the positive observations from the EBD, it doesn’t provide the corresponding Sampling Event Data for zero-filling. However, the Sampling Event Data is much smaller than the EBD and quicker to process with auk. So, we can easily filter this file using the same set of filters we’ve already applied to the EBD. Note that auk_sampling() is used in place of auk_ebd() when we’re only filtering the sampling data and not the EBD. f_sed &lt;- &quot;data/sed-only_amefla.txt&quot; sed_filt &lt;- auk_sampling(&quot;ebd_sampling_2014-2015_yucatan.txt&quot;) %&gt;% auk_state(&quot;MX-YUC&quot;) %&gt;% auk_date(c(&quot;*-01-01&quot;, &quot;*-01-31&quot;)) %&gt;% auk_complete() %&gt;% auk_filter(&quot;data/sed-only_amefla.txt&quot;) sed &lt;- read_sampling(f_sed) Tip This method will only work if the custom download data and the Sampling Event Data are from the same release of the EBD. The EBD is updated monthly, so you’ll want to make sure you download a copy of the Sampling Event Data at the same time as you submit a custom download request. Finally, we can combine these two data frames together with auk_zerofill() to produce zero-filled presence-absence data. ebd_zf &lt;- auk_zerofill(ebd, sed, collapse = TRUE) We’ve produced exactly the same data as in Lesson 6; however, we’ve done so avoiding having to deal with the full EBD! 9.4 Reducing file size Even after filtering the EBD, sometimes it’s still too large to read into R and work with. There are several approaches for reducing the size of the EBD. We’ll cover three here: stricter filtering, removing columns, and splitting by species. 9.4.1 Stricter filtering The most obvious way to reduce the size of an EBD extract is to use stricter filters: focus on a smaller region, shorter time period, or fewer species. To avoid having to waste several hours trying to filter the entire EBD all over again, it’s worthwhile noting that you can always re-filter an EBD extract directly. So, if you realize you were too coarse in your initial filtering, apply the stricter filter to the EBD extract rather than the full EBD so save time. 9.4.2 Removing columns The EBD contains a lot of columns (46 to be precise), many of which are redundant or not useful in most scenarios. For example, country, state, and county each have two columns, one for the name and one for the code. Other columns, such as the Important Bird Area (IBA) that a checklist belong to and the checklist comments, are rarely useful. By removing these columns we can drastically reduce the size of the EBD. The available columns in the EBD are listed and defined in the PDF metadata that comes with the EBD (eBird_Basic_Dataset_Metadata_v1.12.pdf). Alternatively, there’s a useful trick to get a list of column names from an auk_ebd object. ebd &lt;- auk_ebd(&quot;ebd_2014-2015_yucatan.txt&quot;) ebd$col_idx$name #&gt; [1] &quot;global unique identifier&quot; &quot;last edited date&quot; #&gt; [3] &quot;taxonomic order&quot; &quot;category&quot; #&gt; [5] &quot;common name&quot; &quot;scientific name&quot; #&gt; [7] &quot;subspecies common name&quot; &quot;subspecies scientific name&quot; #&gt; [9] &quot;observation count&quot; &quot;breeding bird atlas code&quot; #&gt; [11] &quot;breeding bird atlas category&quot; &quot;age sex&quot; #&gt; [13] &quot;country&quot; &quot;country code&quot; #&gt; [15] &quot;state&quot; &quot;state code&quot; #&gt; [17] &quot;county&quot; &quot;county code&quot; #&gt; [19] &quot;iba code&quot; &quot;bcr code&quot; #&gt; [21] &quot;usfws code&quot; &quot;atlas block&quot; #&gt; [23] &quot;locality&quot; &quot;locality id&quot; #&gt; [25] &quot;locality type&quot; &quot;latitude&quot; #&gt; [27] &quot;longitude&quot; &quot;observation date&quot; #&gt; [29] &quot;time observations started&quot; &quot;observer id&quot; #&gt; [31] &quot;sampling event identifier&quot; &quot;protocol type&quot; #&gt; [33] &quot;protocol code&quot; &quot;project code&quot; #&gt; [35] &quot;duration minutes&quot; &quot;effort distance km&quot; #&gt; [37] &quot;effort area ha&quot; &quot;number observers&quot; #&gt; [39] &quot;all species reported&quot; &quot;group identifier&quot; #&gt; [41] &quot;has media&quot; &quot;approved&quot; #&gt; [43] &quot;reviewed&quot; &quot;reason&quot; #&gt; [45] &quot;trip comments&quot; &quot;species comments&quot; The function auk_select() will process the EBD to only keep the selected columns. cols &lt;- c(&quot;latitude&quot;, &quot;longitude&quot;, &quot;group identifier&quot;, &quot;sampling event identifier&quot;, &quot;scientific name&quot;, &quot;observation count&quot;, &quot;observer_id&quot;) f_select &lt;- &quot;data/ebd_smaller.txt&quot; selected &lt;- auk_select(ebd, select = cols, file = f_select) %&gt;% read_ebd() names(selected) #&gt; [1] &quot;checklist_id&quot; &quot;scientific_name&quot; #&gt; [3] &quot;observation_count&quot; &quot;latitude&quot; #&gt; [5] &quot;longitude&quot; &quot;observer_id&quot; #&gt; [7] &quot;sampling_event_identifier&quot; &quot;group_identifier&quot; auk_select() is typically applied to a subset of the EBD to reduce the size before reading it into R. However, this function can also be applied to entire EBD to remove any columns that you’re sure you’ll never need. If you’re running out of hard drive space, this approach can drastically reduce the size of the EBD and free up space. Exercise Removing columns can also be done concurrently with filtering the EBD. Consult the documentation for auk_filter() and look at the keep and drop arguments. Use these to extract all Least Grebe records from Belize and remove the trip comments and species comments fields in addition to any other fields you feel aren’t required. Solution ebd_leagre &lt;- ebd %&gt;% auk_species(&quot;Least Grebe&quot;) %&gt;% auk_country(&quot;BZ&quot;) %&gt;% auk_filter(&quot;data/ebd_leagre.txt&quot;, drop = c(&quot;trip_comments&quot;, &quot;species_comments&quot;, &quot;country&quot;, &quot;state&quot;, &quot;county&quot;)) %&gt;% read_ebd() 9.4.3 Splitting by species If you’re working with a large number of species, the size of the EBD can quickly increase. An easy way to address this before importing data into R is to split the EBD extract up into separate files, one for each species. You can then read in species one at a time and process them. Note that the Sampling Event Data doesn’t need to be split and the same file can be used for each species EBD file. The function auk_split() facilitates this process. You provide an EBD extract and a vector of species names and it will split the EBD up into species-specific files. In Lesson 8 we extracted eBird data for 3 warbler species, let’s use auk_split() to split this file up. split_species &lt;- c(&quot;Wilson&#39;s Warbler&quot;, &quot;Canada Warbler&quot;, &quot;Pink-headed Warbler&quot;) f_split &lt;- auk_split(&quot;data/ebd_cardellina.txt&quot;, species = split_species, prefix = &quot;data/ebd_split_&quot;) f_split #&gt; [1] &quot;data/ebd_split_Cardellina_pusilla.txt&quot; #&gt; [2] &quot;data/ebd_split_Cardellina_canadensis.txt&quot; #&gt; [3] &quot;data/ebd_split_Cardellina_versicolor.txt&quot; Next, let’s take the first of these files (for Wilson’s Warbler) and zero-fill it using the Sampling Event Data we extracted in Lesson 8. wilwar &lt;- auk_zerofill(f_split[1], sampling_events = &quot;data/sed_cardellina.txt&quot;, collapse = TRUE) When working with the full EBD, this approach is much faster because we only need to filter the EBD once instead of once for each species. This could be the difference between a few hours of processing time and a few days. "],
["model-intro.html", "Lesson 10 Introduction 10.1 Example data", " Lesson 10 Introduction In Part I of this workshop, we saw that eBird provides a wealth of open access bird observation data. The sheer volume of data, combined with the broad spatial, temporal, and taxonomic coverage, make the eBird database a valuable resource for answering a variety of ecological questions. Furthermore, eBird data have two key characteristics that distinguish it from many other citizen science projects and facilitate robust ecological analyses: the complete checklists allow non-detection to be inferred and the effort information associated with a checklist facilitates robust analyses by accounting for variation in the observation process. Despite the strengths of eBird data, species observations collected through citizen science projects present a number of challenges that are not found in conventional scientific data. The following are some of the primary challenges associated these data; challenges that will be addressed in the following lessons: Taxonomic bias: eBirders often have preferences for certain species, which may lead to preferential recording of some species over others. Restricting analyses to complete checklists largely mitigates this issue. Spatial and temporal bias: eBird data are not randomly distributed in space and time. Most eBirders submit data from times and places that are convenient (e.g. near roads or on weekends) or likely to produce checklists with high biodiversity (e.g. in good habitats, during migration, or early in the morning). Spatiotemporal subsampling, covered in Part I of this workshop, can reduce the impact of spatial and temporal bias. Class imbalance: bird species that are rare or hard to detect may have data with high class imbalance, with many more checklists with non-detections than detections. For these species, a distribution model predicting that the species is absent everywhere will have high accuracy, but no ecological value. Subsampling detections and non-detections independently can be used to address class imbalance. Spatial precision: the spatial location of an eBird checklist is given as a single latitude-longitude point; however, this may not be precise for two main reasons. First, for traveling checklists, this location represents just one point on the journey. Second, eBird checklists are often assigned to a hotspot (a common location for all birders visiting a popular birding site) rather than their true location. Filtering eBird data to checklists below a given length (e.g. less than 5 km), and summarizing covariates within a neighborhood around the checklist location, will reduce the impact of spatial imprecision. Variation in detectability: detectability describes the probability of a species that is present in an area being detected and identified. It varies by season, habitat, and species. Furthermore, eBird data are collected with high variation in effort, time of day, number of observers, and external conditions such as weather, all of which can affect the detectability of specie. Therefore, detectability is particularly important to consider when predictions are compared between seasons, habitats or species. Since eBird uses a semi-structured protocol, that collects variables associated with variation in detectability, we’ll be able to account for a larger proportion of this variation in our analyses. The following three lessons will focus on modeling patterns of bird distribution and abundance using eBird data. We won’t delve too deeply into the technical details of these models, rather we’ll focus on demonstrating a set of best practices for distribution modeling that address the specific challenges associated with eBird data. The three types of models that we’ll cover are: Encounter rate: expected rate of an average eBirder encountering a species while traveling 1km, 1 hour, at the optimal time of day. Occupancy: probability that a site hosts one or more individuals of a species. In this model, detection is explicitly estimated. Relative abundance: expected count of a species by an average eBirder while traveling 1km, 1 hour, at the optimal time of day. Both encounter rate and relative abundance will be modeled using a machine learning approach, where the focus is primarily on prediction. In contrast, occupancy modeling is a more traditional likelihood approach, where the focus is on inference and trying to understand the process that generated the data. Machine learning has the additional benefit that it can learn complex, non-linear relationships between the response and predictor variables, while occupancy models are typically constrained to linear effect and simple interactions. 10.1 Example data For all the examples in the following lessons, we’ll focus on modeling Australian Ibis in September in the Temperate and Subtropical Forest Conservation Management Zone 27. To prepare data for these lessons, we extracted complete traveling and stationary eBird checklists from the past 10 years and zero-filled them to produce presence-absence data following the methods outlined in Part I of this workshop. In addition, to reduce the variation in detectability between checklists, we restricted checklists to less than 5 hours long and 5 km in length, and with 10 or fewer observers. Finally, we processed remotely sensed land cover and elevation data to be used as covariates for modeling. Specifically, within a 2.5 km neighborhood around each checklist, we used the MODIS MCD12Q1 land cover product to calculate the percentage of 16 different land cover classes and a global elevation dataset to calculate the mean and standard deviation of elevation. The workshop data package discussed in the Introduction contains all the prepared data. If you haven’t already done so, download and unzip this file, then move all the files in the data/ subdirectory to the data/ subdirectory of your RStudio project. The following map shows the study area and eBird checklists that we’ll use. Note that there are several species in this data package; however, we’ll be focusing on Austrlian Ibis for the following examples. The code used to generate these data is available in the eBird Best Practices book associated with this workshop. Consult the chapter on eBird Data for details on the eBird data extraction and the chapter on habitat covariates for details on preparing the land cover and elevation data. "],
["encounter.html", "Lesson 11 Encounter Rate 11.1 Data preparation 11.2 Random forests 11.3 Habitat associations 11.4 Prediction 11.5 Exercises", " Lesson 11 Encounter Rate In this lesson, we’ll estimate the encounter rate of Austrlian Ibis on eBird checklists in September in the Temperate and Subtropical Forest CMZ, where encounter rate is defined as the probability of an eBirder encountering a species on a standard eBird checklist. We’ll be using random forests in this lesson, a machine learning technique that uses an ensemble of many decision trees, each of which is fit using a bootstrap sampled of the data. For the purposes of this tutorial, we’ll treat the random forest as a black box method. Let’s start by loading all the packages and data we’ll need for this lesson. library(sf) library(raster) library(dggridR) library(lubridate) library(ranger) library(scam) library(PresenceAbsence) library(verification) library(edarf) library(ebirdst) library(fields) library(gridExtra) library(tidyverse) # resolve namespace conflicts select &lt;- dplyr::select projection &lt;- raster::projection map &lt;- purrr::map set.seed(1) # ebird data ebird &lt;- read_csv(&quot;data/ebd_zf_sep_tst.csv&quot;) %&gt;% filter(common_name == &quot;Australian Ibis&quot;) %&gt;% # year required to join to habitat data mutate(year = year(observation_date)) # modis habitat covariates habitat &lt;- read_csv(&quot;data/pland-elev_location-year.csv&quot;) %&gt;% mutate(year = as.integer(year)) # combine ebird and habitat data ebird_habitat &lt;- inner_join(ebird, habitat, by = c(&quot;locality_id&quot;, &quot;year&quot;)) # prediction surface pred_surface &lt;- read_csv(&quot;data/pland-elev_prediction-surface.csv&quot;) r &lt;- raster(&quot;data/prediction-surface.tif&quot;) # load gis data for making maps map_proj &lt;- st_crs(3577) ne_land &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_country&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() cmz &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;cmz&quot;) %&gt;% filter(cmz_name == &quot;Eastern Australia Temperate and Subtropical forests&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_state_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_state_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() 11.1 Data preparation As we learned in Part I of this workshop, spatiotemporal subsampling can reduce spatial and temporal bias, and class imbalance, provided we sample detections and non-detections separately. So, we’ll apply subsampling prior to fitting the random forest model. Tip Sampling detections and non-detections separately will change the prevalence rate of the detections in the data. As a result, the estimated probability of occurrence based on these subsampled data will be larger than the true occurrence rate. When examining the outputs from the models it will be important to recall that we altered the prevalence rate at this stage. # generate hexagonal grid with ~ 5 km betweeen cells dggs &lt;- dgconstruct(spacing = 5) #&gt; Resolution: 13, Area (km^2): 31.9926151554038, Spacing (km): 5.58632116604266, CLS (km): 6.38233997895802 # get hexagonal cell id and week number for each checklist checklist_cell &lt;- ebird_habitat %&gt;% mutate(cell = dgGEO_to_SEQNUM(dggs, longitude, latitude)$seqnum, year = year(observation_date), week = week(observation_date)) # sample one checklist per grid cell per week # sample detection/non-detection independently ebird_ss &lt;- checklist_cell %&gt;% group_by(species_observed, year, week, cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() Exercise For very rare species, a more drastic approach to dealing with class imbalance is needed: only subsampling the non-detections and keeping all the detections. How could you modify the above code to do this? Solution There are several valid ways to code this in R, but here is our approach. split_det &lt;- split(checklist_cell, checklist_cell$species_observed) ebird_all_det &lt;- split_det$`FALSE` %&gt;% group_by(year, week, cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() %&gt;% bind_rows(split_det$`TRUE`) This approach leads to many more detections being kept in the data. sum(ebird_ss$species_observed) #&gt; [1] 1533 sum(ebird_all_det$species_observed) #&gt; [1] 3041 However, some of the extra detections we have with this approach are in the same 5km cell and the same week, they may not be independent. There are trade-offs to many of these decisions about post-hoc sampling. In preparation for modeling, we’ll select only the the columns that will be used as predictors in the model. We include both habitat predictors, which we expect to influence whether a species is present at a site, and also effort predictors to help control for variation in detectability. # select covariates for model ebird_ss &lt;- ebird_ss %&gt;% select(species_observed, year, day_of_year, time_observations_started, duration_minutes, effort_distance_km, number_observers, starts_with(&quot;pland_&quot;), starts_with(&quot;elevation_&quot;)) %&gt;% drop_na() Finally, we’ll hold 20% of the data aside so we have an independent test set, which we can later use to assess the performance of our model. # split 80/20 ebird_split &lt;- ebird_ss %&gt;% split(if_else(runif(nrow(.)) &lt;= 0.8, &quot;train&quot;, &quot;test&quot;)) Exercise How would you modify the above code to include 25% of data in the test set? Solution ebird_split_25 &lt;- ebird_ss %&gt;% split(if_else(runif(nrow(.)) &lt;= 0.75, &quot;train&quot;, &quot;test&quot;)) 11.2 Random forests Random forests are an excellent, general purpose machine learning method suitable for modeling encounter rate in a wide variety of scenarios. To address the issue of class imbalance, we’ll use a balanced random forest approach, a modification of the traditional random forest algorithm specifically designed to handle scenarios in which one class (in our case: species detections) is much more common than the other (non-detections). To implement a balanced random forest, we’ll first need to calculate the frequency of detections (the smaller class). detection_freq &lt;- mean(ebird_split$train$species_observed) Now we can use the ranger package to fit a random forest model to the eBird data. # ranger requires a factor response to do classification ebird_split$train$species_observed &lt;- factor(ebird_split$train$species_observed) # grow random forest rf &lt;- ranger(formula = species_observed ~ ., data = ebird_split$train, importance = &quot;impurity&quot;, probability = TRUE, replace = TRUE, sample.fraction = c(detection_freq, detection_freq)) 11.2.1 Calibration For various reasons, the predicted probabilities from models do not always align with the observed frequencies of detections. We’ll address this mismatch using model calibration, which aligns the estimated probabilities to the observed frequencies. In particular, to calibrate our model results, we predict encounter rate for each checklist in the training set, then fit a binomial Generalized Additive Model (GAM) with the real observations as the response and the predicted encounter rate as the predictor variable. # make predictions on training data occ_pred &lt;- rf$predictions[, 2] # convert the observered response back to a numeric value from factor occ_obs &lt;- ebird_split$train$species_observed %&gt;% as.logical() %&gt;% as.integer() rf_pred_train &lt;- tibble(obs = occ_obs, pred = occ_pred) %&gt;% drop_na() # fit gam calibration model # scam allows us to use constrained shapes for the smooths calibration_model &lt;- scam(obs ~ s(pred, k = 6, bs = &quot;mpi&quot;), gamma = 2, data = rf_pred_train) # plot calibration curve cal_pred &lt;- tibble(pred = seq(0, 1, length.out = 100)) cal_pred &lt;- predict(calibration_model, cal_pred, type = &quot;response&quot;) %&gt;% bind_cols(cal_pred, calibrated = .) ggplot(cal_pred) + aes(x = pred, y = calibrated) + geom_line() + labs(x = &quot;RF prediction&quot;, y = &quot;Calibrated prediction&quot;, title = &quot;Calibration model&quot;) + xlim(0, 1) + ylim(0, 1) 11.2.2 Model assessment To assess model quality, we’ll validate the model’s ability to predict the observed patterns of occurrence using independent validation data (i.e. the 20% test data set that we removed earlier). We’ll use a range of predictive performance metrics to compare the predictions to the actual observations. Different performance metrics reveal different aspects of the data and we can choose to emphasise some over others, depending on the specific goals of our analysis. For example, if we want to minimise the number of false negatives (i.e. we don’t want to miss any places where the species actually occurs), we would focus on sensitivity. # predict on test data using calibrated model p_fitted &lt;- predict(rf, data = ebird_split$test, type = &quot;response&quot;) # extract probability of detection p_fitted &lt;- p_fitted$predictions[, 2] p_calibrated &lt;- predict(calibration_model, newdata = tibble(pred = p_fitted), type = &quot;response&quot;) rf_pred_test &lt;- data.frame(id = seq_along(p_calibrated), # actual detection/non-detection obs = ebird_split$test$species_observed, # uncalibrated prediction fit = p_fitted, # calibrated prediction cal = p_calibrated) %&gt;% # constrain probabilities to 0-1 mutate(cal = pmin(pmax(cal, 0), 1)) %&gt;% drop_na() # mean squared error (mse) mse_fit &lt;- mean((rf_pred_test$obs - rf_pred_test$fit)^2, na.rm = TRUE) mse_cal &lt;- mean((rf_pred_test$obs - rf_pred_test$cal)^2, na.rm = TRUE) # pick threshold to maximize kappa opt_thresh &lt;- optimal.thresholds(rf_pred_test, opt.methods = &quot;MaxKappa&quot;) # calculate accuracy metrics: auc, kappa, sensitivity, specificity, brier metrics_fit &lt;- rf_pred_test %&gt;% select(id, obs, fit) %&gt;% presence.absence.accuracy(threshold = opt_thresh$fit, na.rm = TRUE, st.dev = FALSE) metrics_cal &lt;- rf_pred_test %&gt;% select(id, obs, cal) %&gt;% presence.absence.accuracy(threshold = opt_thresh$cal, na.rm = TRUE, st.dev = FALSE) # combine various performance metrics together rf_assessment &lt;- tibble( model = c(&quot;RF&quot;, &quot;Calibrated RF&quot;), mse = c(mse_fit, mse_cal), sensitivity = c(metrics_fit$sensitivity, metrics_cal$sensitivity), specificity = c(metrics_fit$specificity, metrics_cal$specificity), auc = c(metrics_fit$AUC, metrics_cal$AUC), kappa = c(metrics_fit$Kappa, metrics_cal$Kappa) ) knitr::kable(rf_assessment, digits = 3) model mse sensitivity specificity auc kappa RF 0.163 0.606 0.847 0.82 0.434 Calibrated RF 0.134 0.592 0.855 0.82 0.434 11.3 Habitat associations From the random forest model, we can glean two important sources of information about the association between Austrlian Ibis detection and features of their local environment. First, predictor importance is a measure of the predictive power of each covariate, and is calculated as a byproduct of fitting a random forest model. Second, partial dependence plots estimate the marginal effect of one predictor holding all other predictors constant. 11.3.1 Predictor importance During the process of fitting a random forest model, some variables are removed at each node of the trees that make up the random forest. Predictor importance is based on the mean decrease in accuracy of the model when a given covariate is not used. pi &lt;- enframe(rf$variable.importance, &quot;predictor&quot;, &quot;importance&quot;) # plots ggplot(pi) + aes(x = fct_reorder(predictor, importance), y = importance) + geom_col() + geom_hline(yintercept = 0, size = 2, colour = &quot;#555555&quot;) + scale_y_continuous(expand = c(0, 0)) + coord_flip() + labs(x = NULL, y = &quot;Predictor Importance (Gini Index)&quot;) + theme_minimal() + theme(panel.grid = element_blank(), panel.grid.major.x = element_line(colour = &quot;#cccccc&quot;, size = 0.5)) Tip Consult the file data/mcd12q1_classes.csv for a key to the different pland_ variables. class name 0 Water bodies 1 Evergreen Needleleaf Forests 2 Evergreen Broadleaf Forests 3 Deciduous Needleleaf Forests 4 Deciduous Broadleaf Forests 5 Mixed Forests 6 Closed Shrublands 7 Open Shrublands 8 Woody Savannas 9 Savannas 10 Grasslands 11 Permanent Wetlands 12 Croplands 13 Urban and Built-up Lands 14 Cropland/Natural Vegetation Mosaics 15 Non-Vegetated Lands 255 Unclassified 11.3.2 Partial dependence Partial dependence plots show the marginal effect of a given predictor on encounter rate averaged across the other predictors. We’ll use the R package edarf to construct partial dependence plots for the most important predictors. # top 9 predictors other than date top_pred &lt;- pi %&gt;% filter(!predictor %in% c(&quot;year&quot;, &quot;day_of_year&quot;)) %&gt;% top_n(n = 9, wt = importance) %&gt;% arrange(desc(importance)) # calculate partial dependence for each predictor pd &lt;- top_pred %&gt;% mutate(pd = map(predictor, partial_dependence, fit = rf, data = ebird_split$train), pd = map(pd, ~ .[, c(1, 3)]), pd = map(pd, set_names, nm = c(&quot;value&quot;, &quot;encounter_rate&quot;))) %&gt;% unnest(cols = pd) # calibrate predictions pd$encounter_rate &lt;- predict(calibration_model, newdata = tibble(pred = pd$encounter_rate), type = &quot;response&quot;) %&gt;% as.numeric() # constrain probabilities to 0-1 pd$encounter_rate &lt;- pmin(pmax(pd$encounter_rate, 0), 1) # plot ggplot(pd) + aes(x = value, y = encounter_rate) + geom_line() + geom_point() + scale_y_continuous(labels = scales::percent) + facet_wrap(~ as_factor(predictor), nrow = 3, scales = &quot;free&quot;) + labs(x = NULL, y = &quot;Encounter Rate&quot;) + theme_minimal() + theme_minimal() + theme(panel.grid = element_blank(), axis.line = element_line(color = &quot;grey60&quot;), axis.ticks = element_line(color = &quot;grey60&quot;)) 11.4 Prediction Finally, we can use the calibrated random forest model to make a map of Austrlian Ibis encounter rate in the CMZ! The data package contains a prediction surface consisting of the PLAND habitat covariates summarized on a regular grid of points across the CMZ. We’ll make predictions of encounter rate at these points. However, first we need to bring effort variables into this prediction surface. We’ll make predictions for a standard eBird checklist: a 1 km, 1 hour traveling count at the peak time of day for detecting this species. To find the time of day with the highest detection probability, we can look for the peak of the partial dependence plot, constraining the search to times of day for which there are enough data to make reasonable predictions (hours with at least 1% of checklists). # find peak time of day from partial dependence pd_time &lt;- partial_dependence(rf, vars = &quot;time_observations_started&quot;, # make estimates at 30 minute intervals # use the entire training dataset for estimation n = c(24 * 2, nrow(ebird_split$train)), data = ebird_split$train) %&gt;% select(time_observations_started, encounter_rate = &quot;TRUE&quot;) # hours with at least 1% of checklists search_hours &lt;- ebird_split$train %&gt;% mutate(hour = floor(time_observations_started)) %&gt;% count(hour) %&gt;% mutate(pct = n / sum(n)) %&gt;% filter(pct &gt;= 0.01) # constrained peak time t_peak &lt;- pd_time %&gt;% filter(floor(time_observations_started) %in% search_hours$hour) %&gt;% top_n(1, wt = desc(time_observations_started)) %&gt;% pull(time_observations_started) t_peak #&gt; [1] 5.32 Based on this analysis, the best time for detecting Austrlian Ibis is at 5:19 AM. Now we can use this time to make predictions. # add effort covariates to prediction pred_surface_eff &lt;- pred_surface %&gt;% mutate(observation_date = ymd(&quot;2019-09-15&quot;), year = year(observation_date), day_of_year = yday(observation_date), time_observations_started = t_peak, duration_minutes = 60, effort_distance_km = 1, number_observers = 1) # predict pred_rf &lt;- predict(rf, data = pred_surface_eff, type = &quot;response&quot;) pred_rf &lt;- pred_rf$predictions[, 2] # apply calibration models pred_rf_cal &lt;- predict(calibration_model, data.frame(pred = pred_rf), type = &quot;response&quot;) # add to prediction surface pred_er &lt;- bind_cols(pred_surface_eff, encounter_rate = pred_rf_cal) %&gt;% select(latitude, longitude, encounter_rate) %&gt;% mutate(encounter_rate = pmin(pmax(encounter_rate, 0), 1)) Next, we’ll convert this data frame to spatial features using sf, then rasterize the points using the prediction surface raster template. # rasterize predictions r_pred &lt;- pred_er %&gt;% # convert to spatial features st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% st_transform(crs = projection(r)) %&gt;% # rasterize rasterize(r) r_pred &lt;- r_pred[[-1]] Finally, we can map these predictions! # project predictions r_pred_proj &lt;- projectRaster(r_pred, crs = map_proj$proj4string, method = &quot;ngb&quot;) par(mar = c(0.25, 0.25, 0.25, 4.5)) # set up plot area plot(cmz, col = NA, border = NA) plot(ne_land, col = &quot;#dddddd&quot;, border = &quot;#888888&quot;, lwd = 0.5, add = TRUE) # encounter rate r_max &lt;- ceiling(10 * cellStats(r_pred_proj, max)) / 10 brks &lt;- seq(0, r_max, by = 0.025) lbl_brks &lt;- seq(0, r_max, by = 0.1) # ebird status and trends color palette pal &lt;- abundance_palette(length(brks) - 1) plot(r_pred_proj, col = pal, breaks = brks, maxpixels = ncell(r_pred_proj), legend = FALSE, add = TRUE) # borders plot(ne_state_lines, col = &quot;#ffffff&quot;, lwd = 0.75, add = TRUE) box() # legend par(new = TRUE, mar = c(0, 0, 0, 0)) title &lt;- &quot;Austrlian Ibis Encounter Rate&quot; image.plot(zlim = range(brks), legend.only = TRUE, col = pal, breaks = brks, smallplot = c(0.90, 0.93, 0.25, 0.75), horizontal = FALSE, axis.args = list(at = lbl_brks, labels = lbl_brks, fg = &quot;black&quot;, col.axis = &quot;black&quot;, cex.axis = 0.75, lwd.ticks = 0.5, padj = 0), legend.args = list(text = title, side = 2, col = &quot;black&quot;, cex = 1, line = 0)) 11.5 Exercises Now that you’ve completed this lesson, try modifying your script to complete at least one of the following exercises: How does changing the subsampling grid cell size affect the model performance? What happens to the predictions if you make them for an eBirder traveling further than 1 km, or birding for longer than 1 hour? Filter the data to only shorter duration checklists or shorter distances traveled. How does this affect model performance? An alternative approach to dealing with class imbalance, is to grid sample only the non-detections, while keeping all the detections. Try this subsampling approach and see what the affect is on the predictive performance metrics. Try producing a map of encounter rate using one of the other species in the example data. "],
["occupancy.html", "Lesson 12 Occupancy 12.1 Data preparation 12.2 Occupancy modeling 12.3 Prediction 12.4 Model selection 12.5 Exercises", " Lesson 12 Occupancy In this lesson, we’ll use occupancy models to estimate the occupancy of occupancy of Australian Ibis on eBird checklists in September in the Temperate and Subtropical Forest CMZ, while explicitly accounting for imperfect detection. First, we’ll give a short presentation introducing occupancy modeling. The presentation can be downloaded in PowerPoint or PDF format, or viewed on SpeakerDeck. Let’s start by loading the packages and data required for this lesson. library(auk) library(lubridate) library(sf) library(dggridR) library(unmarked) library(raster) library(ebirdst) library(MuMIn) library(AICcmodavg) library(fields) library(tidyverse) # resolve namespace conflicts select &lt;- dplyr::select projection &lt;- raster::projection set.seed(1) # ebird data ebird &lt;- read_csv(&quot;data/ebd_zf_sep_tst.csv&quot;) %&gt;% filter(common_name == &quot;Australian Ibis&quot;) %&gt;% mutate(year = year(observation_date), # occupancy modeling requires an integer response species_observed = as.integer(species_observed)) # modis land cover covariates habitat &lt;- read_csv(&quot;data/pland-elev_location-year.csv&quot;) %&gt;% mutate(year = as.integer(year)) # combine ebird and modis data ebird_habitat &lt;- inner_join(ebird, habitat, by = c(&quot;locality_id&quot;, &quot;year&quot;)) # prediction surface pred_surface &lt;- read_csv(&quot;data/pland-elev_prediction-surface.csv&quot;) # latest year of landcover data max_lc_year &lt;- pred_surface$year[1] r &lt;- raster(&quot;data/prediction-surface.tif&quot;) # load gis data for making maps map_proj &lt;- st_crs(3577) ne_land &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_country&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() cmz &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;cmz&quot;) %&gt;% filter(cmz_name == &quot;Eastern Australia Temperate and Subtropical forests&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_state_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_state_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() 12.1 Data preparation Since we’ll be fitting a single-season occupancy models, we’ll need to start by focusing on observations from September of a single year, in this case the most recent year for which we have data. At this point, we also suggest subsetting the data to observations with 5 or fewer observers since there are few checklists with more than 5 observers. # filter to a single year of data ebird_filtered &lt;- filter(ebird_habitat, number_observers &lt;= 5, year == max(year)) As we learned in Part I of this workshop, we can use the auk function filter_repeat_visits() to extract just the eBird records that are suitable for occupancy modeling. Specifically, we want repeat visits to the same location by the same observer, so we’ll use latitude, longitude, and observer ID to define ‘sites’. # subset for occupancy modeling occ &lt;- filter_repeat_visits(ebird_filtered, min_obs = 2, max_obs = 10, annual_closure = TRUE, date_var = &quot;observation_date&quot;, site_vars = c(&quot;latitude&quot;, &quot;longitude&quot;, &quot;observer_id&quot;)) Exercise Suppose you define the temporal period of closure as week long blocks, rather than the whole month of September. Use filter_repeat_visits() to extract eBird data accordingly. Solution occ_days &lt;- filter_repeat_visits(ebird_filtered, min_obs = 2, max_obs = 10, n_days = 7, date_var = &quot;observation_date&quot;, site_vars = c(&quot;latitude&quot;, &quot;longitude&quot;, &quot;observer_id&quot;)) Exercise Subsetting eBird data to just the observations suitable for occupancy modeling will inevitably reduce the amount of data. What proportion of observations remain after calling filter_repeat_visits()? How many unique sites do we have? Solution nrow(occ) / nrow(ebird_habitat) #&gt; [1] 0.111 n_distinct(occ$site) #&gt; [1] 392 Next, we’ll use the auk function format_unmarked_occu() to put data in the specific format required by unmarked, as discussed in Part I. Prior knowledge of Australian Ibis, as well as the predictor importance results from the encounter rate lesson, inform what land cover variables we choose as occupancy covariates. The five effort variables should always be included as detection covariates, but we’ll also examine whether different habitat types affect the detection probability. # format for unmarked, select occupancy and detection covariates occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;species_observed&quot;, site_covs = c(&quot;n_observations&quot;, &quot;latitude&quot;, &quot;longitude&quot;, # % urban &quot;pland_13&quot;, # % savanna &quot;pland_09&quot;, # % evergreen broadleaf forest &quot;pland_02&quot;, # % woody savanna &quot;pland_08&quot;), obs_covs = c(&quot;time_observations_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;pland_13&quot;, &quot;pland_09&quot;)) As described in lesson 7, we’ll use spatial subsampling to reduce spatial bias. However, here we’ll subsample at the level of ‘sites’ rather than observations. # generate hexagonal grid with ~ 5 km betweeen cells dggs &lt;- dgconstruct(spacing = 5) # get hexagonal cell id for each site occ_wide_cell &lt;- occ_wide %&gt;% mutate(cell = dgGEO_to_SEQNUM(dggs, longitude, latitude)$seqnum) # sample one checklist per grid cell occ_ss &lt;- occ_wide_cell %&gt;% group_by(cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() %&gt;% select(-cell) Finally, we’ll convert this data frame of observations into an unmarked object in order to fit occupancy models. # creat unmarked object occ_um &lt;- formatWide(occ_ss, type = &quot;unmarkedFrameOccu&quot;) 12.2 Occupancy modeling Now that the data are prepared, we can fit a single-season occupancy model to using the occu() function, specifying the detection and occupancy covariates, respectively, via a double right-hand sided formula of the form ~ detection covariates ~ occupancy covariates. # fit model occ_model &lt;- occu(~ time_observations_started + duration_minutes + effort_distance_km + number_observers + protocol_type + pland_02 + pland_08 ~ pland_13 + pland_09 + pland_02 + pland_08, data = occ_um) # look at the regression coefficients from the model summary(occ_model) #&gt; #&gt; Call: #&gt; occu(formula = ~time_observations_started + duration_minutes + #&gt; effort_distance_km + number_observers + protocol_type + pland_02 + #&gt; pland_08 ~ pland_13 + pland_09 + pland_02 + pland_08, data = occ_um) #&gt; #&gt; Occupancy (logit-scale): #&gt; Estimate SE z P(&gt;|z|) #&gt; (Intercept) -0.621 0.501 -1.240 0.21484 #&gt; pland_13 0.911 0.644 1.416 0.15690 #&gt; pland_09 0.420 0.831 0.506 0.61303 #&gt; pland_02 -3.013 1.122 -2.687 0.00722 #&gt; pland_08 12.563 5.675 2.214 0.02685 #&gt; #&gt; Detection (logit-scale): #&gt; Estimate SE z P(&gt;|z|) #&gt; (Intercept) -0.17286 0.66587 -0.2596 7.95e-01 #&gt; time_observations_started -0.02826 0.04196 -0.6735 5.01e-01 #&gt; duration_minutes 0.01648 0.00472 3.4936 4.76e-04 #&gt; effort_distance_km 0.04264 0.16428 0.2595 7.95e-01 #&gt; number_observers -0.38447 0.21758 -1.7670 7.72e-02 #&gt; protocol_typeTraveling 0.61871 0.45763 1.3520 1.76e-01 #&gt; pland_02 0.00581 1.11851 0.0052 9.96e-01 #&gt; pland_08 -11.29335 2.34432 -4.8173 1.45e-06 #&gt; #&gt; AIC: 468 #&gt; Number of sites: 208 #&gt; optim convergence code: 0 #&gt; optim iterations: 66 #&gt; Bootstrap iterations: 0 12.2.1 Assessment The MacKenzie and Bailey [-@mackenzieAssessingFitSiteoccupancy2004] goodness-of-fit test can be used to assess the occupancy model fit. Note that to produce accurate results, this process requires simulating about 1,000 bootstrap samples, which can take a long time to run. For the sake of speed, if you want to run the below code, we suggest using nsim = 5. occ_gof &lt;- mb.gof.test(occ_model, nsim = 1000, plot.hist = FALSE) print(occ_gof) #&gt; #&gt; MacKenzie and Bailey goodness-of-fit for single-season occupancy model #&gt; #&gt; Chi-square statistic = 612 #&gt; Number of bootstrap samples = 1000 #&gt; P-value = 0.989 #&gt; #&gt; Quantiles of bootstrapped statistics: #&gt; 0% 25% 50% 75% 100% #&gt; 451 1248 1609 2072 26914 #&gt; #&gt; Estimate of c-hat = 0.31 12.3 Prediction Now we can estimate the distribution of Australian Ibis in the CMZ and produce a map. Recall that when we predicted encouter rate, we had to include effort variables in our prediction surface. We don’t need to do that here because the estimated occupancy doesn’t depend on the effort covariates, these only occur in the detection submodel. In addition, predict() can produce both predictions as well as estimates of the standard error. # make prediction occ_pred &lt;- predict(occ_model, newdata = as.data.frame(pred_surface), type = &quot;state&quot;) # add to prediction surface pred_occ &lt;- bind_cols(pred_surface, occ_prob = occ_pred$Predicted, occ_se = occ_pred$SE) %&gt;% select(latitude, longitude, occ_prob, occ_se) Checkpoint Predicting on the full prediction surface will typically take several minutes. As the above code runs, let’s take a short break. Next, we want to plot these predictions. We’ll convert this data frame to spatial features using sf, then rasterize the points using the prediction surface raster template. r_pred &lt;- pred_occ %&gt;% # convert to spatial features st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% st_transform(crs = projection(r)) %&gt;% # rasterize rasterize(r) r_pred &lt;- r_pred[[c(&quot;occ_prob&quot;, &quot;occ_se&quot;)]] Finally, we can map these predictions! # project predictions r_pred_proj &lt;- projectRaster(r_pred, crs = map_proj$proj4string, method = &quot;ngb&quot;) par(mfrow = c(2, 1)) for (nm in names(r_pred)) { r_plot &lt;- r_pred_proj[[nm]] par(mar = c(0.25, 0.25, 0.25, 5)) # set up plot area plot(cmz, col = NA, border = NA) plot(ne_land, col = &quot;#dddddd&quot;, border = &quot;#888888&quot;, lwd = 0.5, add = TRUE) # occupancy probability or standard error if (nm == &quot;occ_prob&quot;) { title &lt;- &quot;Australian Ibis Occupancy Probability&quot; brks &lt;- seq(0, 1, length.out = 21) lbl_brks &lt;- seq(0, 1, length.out = 11) %&gt;% round(2) } else { title &lt;- &quot;Australian Ibis Occupancy Uncertainty (SE)&quot; mx &lt;- ceiling(1000 * cellStats(r_plot, max)) / 1000 brks &lt;- seq(0, mx, length.out = 21) lbl_brks &lt;- seq(0, mx, length.out = 11) %&gt;% round(3) } pal &lt;- abundance_palette(length(brks) - 1) plot(r_plot, col = pal, breaks = brks, maxpixels = ncell(r_plot), legend = FALSE, add = TRUE) # borders plot(ne_state_lines, col = &quot;#ffffff&quot;, lwd = 0.75, add = TRUE) box() # legend par(new = TRUE, mar = c(0, 0, 0, 0)) image.plot(zlim = range(brks), legend.only = TRUE, breaks = brks, col = pal, smallplot = c(0.89, 0.92, 0.25, 0.75), horizontal = FALSE, axis.args = list(at = lbl_brks, labels = lbl_brks, fg = &quot;black&quot;, col.axis = &quot;black&quot;, cex.axis = 0.75, lwd.ticks = 0.5, padj = 0), legend.args = list(text = title, side = 2, col = &quot;black&quot;, cex = 1, line = 0)) } 12.4 Model selection So far, we’ve been using a global model that includes all of the covariates we believe will influence the occupancy and detection probabilities; however, a more thorough approach is to use model selection to compare and rank a set of candidate models, each containing different subsets of the covariates. We’ll use the dredge() function, which evaluates a set of candidate models generated by using different combinations of the terms in the global model. Since we know from prior experience that the effort covariates are almost always important, we’ll lock these variables in, and consider a candidate set consisting of all possible combinations of the ecological covariates in the occupancy submodel. # get list of all possible terms, then subset to those we want to keep det_terms &lt;- getAllTerms(occ_model) %&gt;% # retain the detection submodel covariates discard(str_detect, pattern = &quot;psi&quot;) # fit all possible combinations of the occupancy covariates occ_dredge &lt;- dredge(occ_model, fixed = det_terms) # model comparison select(occ_dredge, starts_with(&quot;psi(p&quot;), df, AICc, delta, weight) %&gt;% mutate_all(~ round(., 3)) %&gt;% knitr::kable() psi(pland_02) psi(pland_08) psi(pland_09) psi(pland_13) df AICc delta weight -3.56 11.06 11 468 0.000 0.329 -3.18 12.63 0.733 12 468 0.365 0.274 -3.60 11.31 -0.225 12 470 2.147 0.112 -3.01 12.56 0.420 0.911 13 470 2.383 0.100 -2.90 10 471 3.014 0.073 -2.67 0.382 11 472 4.680 0.032 -2.89 0.106 11 473 5.218 0.024 11.71 1.075 11 474 6.323 0.014 -2.43 0.568 0.635 12 474 6.415 0.013 11.15 0.891 1.457 12 475 7.456 0.008 8.19 10 476 8.348 0.005 9 476 8.358 0.005 0.692 10 476 8.550 0.005 0.941 1.107 11 477 9.383 0.003 8.59 -0.191 11 478 10.499 0.002 0.082 10 478 10.549 0.002 Tip To determine which candidate model each row corresponds to, note that he columns beginning with psi( give the model coefficients for each of the habitat covariates in the occupancy submodel; missing values indicate that that covariate was not included in the given model. The corrected Akaike Information Criterion (AICc) measures the performance of each model, relative to the other models in the candidate set, adjusting for the number of parameters. Lower values indicate models with a better fit to the data, penalizing for the number of parameters. Delta is the difference between the AICc values for the given model and that for the top model (i.e. the one with the lowest AICc). Finally, the AIC weight is a transformation of delta that can be interpreted as the likelihood that the given model is the most likely one of the candidate models to have generated the data. There doesn’t appear to be a clear single model, or even a small set of models, that are most likely to have generated our data. Given this, we’ll average across the top models (those comprising 95% of the weights), weighted by AICc, to produce a model-averaged prediction. # select models with the most suport for model averaging occ_dredge_95 &lt;- get.models(occ_dredge, subset = cumsum(weight) &lt;= 0.95) # average models based on model weights occ_avg &lt;- model.avg(occ_dredge_95, fit = TRUE) Making model-averaged predictions works in the same way as for making predictions from a single global model. However, it takes significantly more time to run, so we won’t run the below code or produce maps based on model-averaged predictions during the workshop, instead leaving it as an exercise. # occ_pred_avg &lt;- predict(occ_avg, newdata = as.data.frame(pred_surface), type = &quot;state&quot;) # add to prediction surface pred_occ_avg &lt;- bind_cols(pred_surface, occ_prob = occ_pred_avg$fit, occ_se = occ_pred_avg$se.fit) %&gt;% select(latitude, longitude, occ_prob, occ_se) 12.4.1 Detection model A unique feature of occupancy models, is that we can investigate whether certain covariates influence detection, which wasn’t possible using the machine learning approach in lesson 11. We included evergreen broadleaf forest (pland_02) and woody savanna (pland_08) as detection covariates in the global model, and we can compare a set of models with and without these covariates to assess their importance. # define occupancy model formula with only effort in detection submodel det_mod &lt;- ~ time_observations_started + duration_minutes + effort_distance_km + number_observers + protocol_type ~ pland_13 + pland_09 + pland_02 + pland_08 # create new formulae with landcover covariates added to the detection submodel mods &lt;- list(det_mod_null = det_mod, det_mod_for = update.formula(det_mod, ~ . + pland_02 ~ .), det_mod_ws = update.formula(det_mod, ~ . + pland_08 ~ .), global = update.formula(det_mod, ~ . + pland_02 + pland_08 ~ .)) %&gt;% # fit candidate models map(occu, data = occ_um) Now we can perform model selection on this set to compare the different candidate models. mod_sel &lt;- fitList(fits = mods) %&gt;% modSel() mod_sel #&gt; nPars AIC delta AICwt cumltvWt #&gt; det_mod_ws 12 466.16 0.00 0.73060 0.73 #&gt; global 13 468.16 2.00 0.26878 1.00 #&gt; det_mod_null 11 481.39 15.22 0.00036 1.00 #&gt; det_mod_for 12 482.08 15.91 0.00026 1.00 From these results, it’s clear that including these habitat covariates (especially woody savanna) leads to an improvement in model performance, as shown by the AIC and AIC weights. Let’s look at the coefficients from the global model for these covariates to see how they’re impacting detection and occupancy. coef(occ_model) %&gt;% enframe() %&gt;% filter(str_detect(name, &quot;pland_&quot;)) #&gt; # A tibble: 6 x 2 #&gt; name value #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 psi(pland_13) 0.911 #&gt; 2 psi(pland_09) 0.420 #&gt; 3 psi(pland_02) -3.01 #&gt; 4 psi(pland_08) 12.6 #&gt; 5 p(pland_02) 0.00581 #&gt; 6 p(pland_08) -11.3 The psi() coefficients are from the occupancy submodel and the p() coefficients are from the detection submodel. 12.5 Exercises Now that you’ve completed this lesson, try modifying your script to complete at least one of the following exercises: Try sampling more than a single checklist per grid cell in the spatiotemporal sampling. How does that affect model fit and predictions? What happens to the size of dataset if you only use stationary counts, or reduce the distance traveled to 1 km? How does it impact the results? How does the different input data affect your interpretation of the results? What happens to the size of the dataset if you allow repeat visits to be by multiple observers? How does this impact the results. Produce a map based on model averaged predictions. Note that making these predictions may take up to an hour. Try estimating occupancy for one of the other species in the example data. "],
["abundance.html", "Lesson 13 Abundance 13.1 Data preparation 13.2 Exploratory data analysis 13.3 Abundance models 13.4 Asssessment 13.5 Prediction 13.6 Exercises", " Lesson 13 Abundance The previous two lessons focused on modeling occurrence, which was based on presence-absence data. However, in addition to recording which species they observed, most eBirders also specify how many individuals of each species were observed. In this lesson, we’ll take advantage of these counts to model relative abundance. So far the lessons have covered random forests with the encounter rate models and occupancy models. Here we use a new type of model to describe variation in relative abundance Generalized Additive Models (GAMs). We will not describe these models in this lesson, rather just use them as a tool. In your own time, there are several resources on the internet to learn more about GAMs. Let’s start by loading the packages and data required for this lesson. Note that, unlike the previous lessons, we need to subset the eBird data to remove records for which the observer recorded the presence of Australian Ibis, but didn’t record the number of individuals. library(lubridate) library(sf) library(raster) library(dggridR) library(pdp) library(edarf) library(mgcv) library(fitdistrplus) library(viridis) library(fields) library(tidyverse) # resolve namespace conflicts select &lt;- dplyr::select map &lt;- purrr::map projection &lt;- raster::projection set.seed(1) # ebird data ebird &lt;- read_csv(&quot;data/ebd_zf_sep_tst.csv&quot;) %&gt;% filter(common_name == &quot;Australian Ibis&quot;) %&gt;% mutate(protocol_type = factor(protocol_type, levels = c(&quot;Stationary&quot; , &quot;Traveling&quot;))) %&gt;% # remove observations with no count filter(!is.na(observation_count)) # modis habitat covariates habitat &lt;- read_csv(&quot;data/pland-elev_location-year.csv&quot;) %&gt;% mutate(year = as.integer(year)) # combine ebird and habitat data ebird_habitat &lt;- inner_join(ebird, habitat, by = c(&quot;locality_id&quot;, &quot;year&quot;)) # prediction surface pred_surface &lt;- read_csv(&quot;data/pland-elev_prediction-surface.csv&quot;) # latest year of landcover data max_lc_year &lt;- pred_surface$year[1] r &lt;- raster(&quot;data/prediction-surface.tif&quot;) # load gis data for making maps map_proj &lt;- st_crs(3577) ne_land &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_country&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() cmz &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;cmz&quot;) %&gt;% filter(cmz_name == &quot;Eastern Australia Temperate and Subtropical forests&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() ne_state_lines &lt;- read_sf(&quot;data/gis-data.gpkg&quot;, &quot;ne_state_lines&quot;) %&gt;% st_transform(crs = map_proj) %&gt;% st_geometry() Exercise Since we’re modeling abundance, we had to remove checklists for which the observer didn’t include a count of the number of Australian Ibis. How many checklists didn’t have counts? What proportion of the data is this? Solution read_csv(&quot;data/ebd_zf_sep_tst.csv&quot;) %&gt;% filter(common_name == &quot;Australian Ibis&quot;) %&gt;% summarize(n_total = n(), n_nocount = sum(is.na(observation_count)), prop = mean(is.na(observation_count))) #&gt; # A tibble: 1 x 3 #&gt; n_total n_nocount prop #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 12311 203 0.0165 13.1 Data preparation As we learned in Part I of this workshop, spatiotemporal subsampling can reduce spatial and temporal bias, and class imbalance, provided we sample detections and non-detections separately. So, we’ll apply subsampling prior to fitting the relative abundance model. # generate hexagonal grid with ~ 5 km betweeen cells dggs &lt;- dgconstruct(spacing = 5) # get hexagonal cell id and week number for each checklist checklist_cell &lt;- ebird_habitat %&gt;% mutate(cell = dgGEO_to_SEQNUM(dggs, longitude, latitude)$seqnum, week = week(observation_date)) # sample one checklist per grid cell per week # sample detection/non-detection independently ebird_ss &lt;- checklist_cell %&gt;% group_by(species_observed, year, week, cell) %&gt;% sample_n(size = 1) %&gt;% ungroup() %&gt;% select(-cell, -week) Finally, we hold 20% of the data aside so we have an independent test set, which we can later use to assess the performance of our model. # split 80/20 ebird_split &lt;- ebird_ss %&gt;% split(if_else(runif(nrow(.)) &lt;= 0.8, &quot;train&quot;, &quot;test&quot;)) 13.2 Exploratory data analysis We’ll start by examining the distribution of the count data to get an idea of which error distributions may be appropriate for the GAM model. We’ll also be on the lookout for evidence of zero-inflation, which can arise becuase eBird data often have a very high number of zero counts, since even common bird species are not seen on every checklist. par(mfrow = c(1, 2)) # counts with zeros hist(ebird_ss$observation_count, main = &quot;Histogram of counts&quot;, xlab = &quot;Observed count&quot;) # counts without zeros pos_counts &lt;- keep(ebird_ss$observation_count, ~ . &gt; 0) hist(pos_counts, main = &quot;Histogram of counts &gt; 0&quot;, xlab = &quot;Observed non-zero count&quot;) prop_zero &lt;- sum(ebird_ss$observation_count == 0) / nrow(ebird_ss) prop_zero #&gt; [1] 0.76 After the subsampling, 76.0% of the counts are zeros. The plot that includes zeros (top row) show an extremely zero-inflated and skewed distribution, due to the large number of zero-counts (checklists with no Australian Ibis detections). With zeros removed there is still a distribution of counts with a strong skew. 13.3 Abundance models We’ll be using Generalized Additive Models (GAMs) to model relative abundance. GAMs can flexibly include many covariates while offering a choice of several error distributions suitable for count responses. Prior to fitting a GAM model, we need to construct the model formula. We’ll allow the effect of each individual continuous covariate to vary smoothly across the range its values by specifying a spline smooth with four degrees of freedom (k = 5). The one variable with a different smooth is checklist start time, which is a cyclic variable (i.e. 0 and 24 are both midnight), so we’ll use a cubic cyclic spline (bs = &quot;cc&quot;) with six degrees of freedom (k = 7). We include all the effort covariates and four land cover covariates. Deciduous broadleaf forest (pland_04) and mixed forest (pland_05) are known Australian Ibis breeding habitat, and they are known to avoid are croplands (pland_12) and urban (pland_13). # gam formula k_time &lt;- 7 gam_formula &lt;- observation_count ~ s(day_of_year, k = 5) + s(duration_minutes, k = 5) + s(effort_distance_km, k = 5) + s(number_observers, k = 5) + s(pland_13, k = 5) + s(pland_09, k = 5) + s(pland_02, k = 5) + s(pland_08, k = 5) + protocol_type + s(time_observations_started, bs = &quot;cc&quot;, k = k_time) # explicitly specify where the knots should occur for time_observations_started # this ensures that the cyclic spline joins the variable at midnight # this won&#39;t happen by default if there are no data near midnight time_knots &lt;- list(time_observations_started = seq(0, 24, length.out = k_time)) Exercise Modify the above code to include the two elevation covariates in the model formula. Solution gam_formula_elev &lt;- observation_count ~ s(day_of_year, k = 5) + s(duration_minutes, k = 5) + s(effort_distance_km, k = 5) + s(number_observers, k = 5) + s(pland_13, k = 5) + s(pland_09, k = 5) + s(pland_02, k = 5) + s(pland_08, k = 5) + s(elevation_median, k = 5) + s(elevation_sd, k = 5) + protocol_type + s(time_observations_started, bs = &quot;cc&quot;, k = k_time) Now we’ll use this formula to fit GAM models. It can be hard to judge in advance of model fitting which distribution will best describe the count data. So we will testing three count response distributions: Zero-inflated Poisson: This distribution effectively fits the data in two parts: (1) a binomial model that determines the variables associated with species presence and (2) a Poisson count model for those places with species presence, that determines the variables associated with species count. This is an effective distribution when there are a large number of zero counts in the data and the positive counts approximate a Poisson distribution. Negative binomial: The negative binomial distribution is related to the Poisson distribution. However, the Poisson distribution has the variance of the distribution equal to the mean, while the variance can be considerably different from the mean in the negative binomorial distribution. This distribution is appropriate for over-dispersed data when the variance is much larger than the mean—a situation called over-dispersion that is very common in ecological count data. Tweedie distribution: This is a very flexible distribution that encompasses a wide variety of shapes, including those with extremely high variance relative to the mean and extreme over-dispersion. The Tweedie distribution fit in GAM spans a range of distributions from the Poisson to the gamma. # zero-inflated poisson m_ziplss &lt;- gam(list(gam_formula, # count model gam_formula[-2]), # presence model data = ebird_split$test, family = &quot;ziplss&quot;, knots = time_knots) # negative binomial m_nb &lt;- gam(gam_formula, data = ebird_split$train, family = &quot;nb&quot;, knots = time_knots) # tweedie distribution m_tw &lt;- gam(gam_formula, data = ebird_split$train, family = &quot;tw&quot;, knots = time_knots) 13.4 Asssessment To assess the predictive performance of these models, we start by predicting relative abundance on the test data set that we held aside from model fitting. We’ll do this separately for each distribution and compare their performance. Care needs to be taken when making predictions from the zero-inflated Poisson model since it has two components: probability of presence and expected count given presence. As a result, the predict() function returns a two column matrix with the count and probability respectively, both on the scale of the link functions. So, we need to back-transform these values, then multiply them to get the predicted counts. obs_count &lt;- select(ebird_split$test, obs = observation_count) # presence probability is on the complimentary log-log scale # we can get the inverse link function with inv_link &lt;- binomial(link = &quot;cloglog&quot;)$linkinv # combine ziplss presence and count predictions m_ziplss_pred &lt;- predict(m_ziplss, ebird_split$test, type = &quot;link&quot;) %&gt;% as.data.frame() %&gt;% transmute(family = &quot;Zero-inflated Poisson&quot;, pred = inv_link(V2) * exp(V1)) %&gt;% bind_cols(obs_count) m_nb_pred &lt;- predict(m_nb, ebird_split$test, type = &quot;response&quot;) %&gt;% tibble(family = &quot;Negative Binomial&quot;, pred = .) %&gt;% bind_cols(obs_count) m_tw_pred &lt;- predict(m_tw, ebird_split$test, type = &quot;response&quot;) %&gt;% tibble(family = &quot;Tweedie&quot;, pred = .) %&gt;% bind_cols(obs_count) # combine predictions from all three models test_pred &lt;- bind_rows(m_ziplss_pred, m_nb_pred, m_tw_pred) %&gt;% mutate(family = as_factor(family)) Assessing the fit of the models depends considerably on the goals of the model and hence on the most important aspects of the model fit for the intended use. Here we’ll calculate three different metrics, each of which is suitable for assessing performance for a different intended use. There are a variety of other metrics that can be used, but the metrics used to assess fit should be carefully chosen with the model goals in mind. Spearman’s Rank Correlation** can be used to assess how each model performs in terms of the ranking of predicted counts. This can be most valuable if we want to understand which sites have highest abundance of the species. Mean absolute deviation (MAD) is a robust statistic that describes the average deviation between observation and prediction, which gives an overall sense of the quality of the magnitude of the predicted counts. This can be most valuable if we want to understand how close our estimates of counts are to the observed counts. In some cases, we may be particularly concerned with underestimating abundances. The number and proportion of predictions that are underestimated by an order of magnitude is one means of assessing performance in this respect. This can be most valuable if we want to understand how prone our model may be to underestimating counts. # spearman’s rank correlation test_pred %&gt;% group_by(family) %&gt;% summarise(rank_cor = cor.test(obs, pred, method = &quot;spearman&quot;, exact = FALSE)$estimate, mad = mean(abs(obs - pred), na.rm = TRUE), n_under = sum(obs / pred &gt; 10), pct_under = mean(obs / pred &gt; 10)) %&gt;% ungroup() %&gt;% knitr::kable(digits = 4) family rank_cor mad n_under pct_under Zero-inflated Poisson 0.176 47.53 53 0.0433 Negative Binomial 0.355 3.76 29 0.0237 Tweedie 0.353 3.77 31 0.0253 Across all metrics, the zero-inflated Poisson model performs the worst: it has the lowest rank correlation, largest number of problematic errors, and the highest MAD. The negative binomial and Tweedie models are very similar in performance, with the negative binomial being slightly better. Exercise Imagine for your use case, underestimation is not a major problem, rather you want to avoid overestimating abundances. Summarize the test set predictions to indentify the number and proportion of times each model overestimates abundance by at least an order of magnitude. Ignore cases where the observed abundance was zero. Solution test_pred %&gt;% filter(obs &gt; 0) %&gt;% group_by(family) %&gt;% summarise(n_under = sum(obs / pred &lt; 10), pct_under = mean(obs / pred &lt; 10)) %&gt;% ungroup() #&gt; # A tibble: 3 x 3 #&gt; family n_under pct_under #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Zero-inflated Poisson 233 0.815 #&gt; 2 Negative Binomial 257 0.899 #&gt; 3 Tweedie 255 0.892 We can also assess the quality of the magnitude of abundance estimates by looking at a plot of predicted and observerved counts from the test set. We’ll highlight in red the regions where the predictions are underestimated by more than an order of magnitude. We’ll also overlay the line \\(y = x\\), which separates overestimates (above the line) from underestimates (below the line), and a blue smoothed fit showing the general trend through all the data. # plot predicted vs. observed ticks &lt;- c(0, 1, 10, 100, 1000) mx &lt;- round(max(test_pred$obs)) ggplot(test_pred) + aes(x = log10(obs + 1), y = log10(pred + 1)) + geom_jitter(alpha = 0.2, height = 0) + # y = x line geom_abline(slope = 1, intercept = 0, alpha = 0.5) + # area where counts off by a factor of 10 geom_area(data = tibble(x = log10(seq(0, mx - 1) + 1), y = log10(seq(0, mx - 1) / 10 + 1)), mapping = aes(x = x, y = y), fill = &quot;red&quot;, alpha = 0.2) + # loess fit geom_smooth(method = &quot;loess&quot;, method.args = list(span = 2 / 3, degree = 1)) + scale_x_continuous(breaks = log10(ticks + 1), labels = ticks) + scale_y_continuous(breaks = log10(ticks + 1), labels = ticks) + labs(x = &quot;Observed count&quot;, y = &quot;Predicted count&quot;) + facet_wrap(~ family, nrow = 1) Overall, we see that most of the counts are underestimated, since most points and the blue line are below the gray \\(y = x\\) line. And, of these, many are underestimated by an order of magnitude or more. Going forward, we should be aware that many of the estimates of relative abundance are likely to be underestimates. This can be a common model trait when the data are overdispersed or have many zeroes. Taking a holistic view of all the three metrics, the negative binomial appears to be best. pred_model &lt;- m_nb Depending on your focal species and region, as well as the particular goals of your analysis, some aspects of model fit will be more important than others, so it’s important to consider your assessment criteria and make sure they match your application. 13.4.1 Assessing covariate effects We recommend assessing the fitted covariate effects to see whether they show biologically plausible relationships between covariates and species counts. Splines can sometimes overfit, notably when sample sizes are very large, and in this cases it is appropriate to reduce the degrees of freedom. Calling the plot() function on the GAM object produces plots of the smooth functions for each of the separate predictors, which gives us a sense of the effect of each predictor on the count response. par(mai = c(0.75, 0.25, 0.2, 0.25)) plot(pred_model, pages = 1, ylab = &quot;&quot;) If these relationships seem too wiggly to be biologically realistic, you should reduce the degrees of freedom for the problematic smooths until a biologically feasible relationship is achieved. In this case, the relationships appear to be reasonable for the negative binomial model, so we will use it for prediction below. 13.5 Prediction Finally, once we have a model we are happy with, we can use the GAM to make a map of Australian Ibis relative abundance in the Temperature and Subtropical Forest CMZ! The data package contains a prediction surface consisting of the PLAND habitat covariates summarized on a regular grid of points across the CMZ. We’ll make predictions of relative abundance at all these points. However, first we need to bring effort variables into this prediction surface. As we did for encounter rate, we’ll make predictions for a standard eBird checklist: a 1 km, 1 hour traveling count at the peak time of day for detecting this species. To find the time of day with the highest detection probability, we’ll predict abundance and its 95% confidence limits at a series of times throughout the day, then pick the time at which the lower confidence limit is at its maximum. By using the lower confidence limits, we select a time that we are confident has high detectability and thus avoid potentially unrealistic predictions from times of day for which few or no data existed (which is more of a risk with GAMs). # create a dataframe of covariates with a range of start times seq_tod &lt;- seq(0, 24, length.out = 300) tod_df &lt;- ebird_split$train %&gt;% # find average pland habitat covariates select(starts_with(&quot;pland&quot;)) %&gt;% summarize_all(mean, na.rm = TRUE) %&gt;% ungroup() %&gt;% # use standard checklist mutate(day_of_year = yday(ymd(str_glue(&quot;{max_lc_year}-06-15&quot;))), duration_minutes = 60, effort_distance_km = 1, number_observers = 1, protocol_type = &quot;Traveling&quot;) %&gt;% cbind(time_observations_started = seq_tod) # predict at different start times pred_tod &lt;- predict(pred_model, newdata = tod_df, type = &quot;link&quot;, se.fit = TRUE) %&gt;% as_tibble() %&gt;% # calculate backtransformed confidence limits transmute(time_observations_started = seq_tod, pred = pred_model$family$linkinv(fit), pred_lcl = pred_model$family$linkinv(fit - 1.96 * se.fit), pred_ucl = pred_model$family$linkinv(fit + 1.96 * se.fit)) # find optimal time of day t_peak &lt;- pred_tod$time_observations_started[which.max(pred_tod$pred_lcl)] # plot the partial dependence plot ggplot(pred_tod) + aes(x = time_observations_started, y = pred, ymin = pred_lcl, ymax = pred_ucl) + geom_ribbon(fill = &quot;grey80&quot;, alpha = 0.5) + geom_line() + geom_vline(xintercept = t_peak, color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + labs(x = &quot;Hours since midnight&quot;, y = &quot;Predicted relative abundance&quot;, title = &quot;Effect of observation start time on Australian Ibis reporting&quot;, subtitle = &quot;Peak detectability shown as dashed blue line&quot;) So, the peak time of day for detecting Australian Ibis is around 18:28 PM. Now we can add all the effort covariates to the prediciton surface. # add effort covariates to prediction surface pred_surface_eff &lt;- pred_surface %&gt;% mutate(day_of_year = yday(ymd(str_glue(&quot;{max_lc_year}-06-15&quot;))), time_observations_started = t_peak, duration_minutes = 60, effort_distance_km = 1, number_observers = 1, protocol_type = &quot;Traveling&quot;) Then estimate relative abundance, standard error, and 95% confidence limits at these points. # predict pred &lt;- predict(pred_model, newdata = pred_surface_eff, type = &quot;link&quot;, se.fit = TRUE) %&gt;% as_tibble() %&gt;% # calculate confidence limits and back transform transmute(abd = pred_model$family$linkinv(fit), abd_se = pred_model$family$linkinv(se.fit), abd_lcl = pred_model$family$linkinv(fit - 1.96 * se.fit), abd_ucl = pred_model$family$linkinv(fit + 1.96 * se.fit)) %&gt;% # add to prediction surface bind_cols(pred_surface_eff, .) %&gt;% select(latitude, longitude, abd, abd_se, abd_lcl, abd_ucl) Next, we’ll convert this data frame to spatial features using sf, then rasterize the points using the prediction surface raster template. r_pred &lt;- pred %&gt;% # convert to spatial features st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% select(abd, abd_se) %&gt;% st_transform(crs = projection(r)) %&gt;% # rasterize rasterize(r) r_pred &lt;- r_pred[[-1]] Finally, let’s make maps of both relative abundance and its standard error! For the relative abundance map, we’ll treat very small values of relative abundance as zero. # any expected abundances below this threshold are set to zero zero_threshold &lt;- 0.05 # project predictions r_pred_proj &lt;- projectRaster(r_pred, crs = map_proj$proj4string, method = &quot;ngb&quot;) par(mfrow = c(2, 1)) for (nm in names(r_pred)) { r_plot &lt;- r_pred_proj[[nm]] par(mar = c(0.25, 0.25, 0.25, 5)) # set up plot area plot(cmz, col = NA, border = NA) plot(ne_land, col = &quot;#dddddd&quot;, border = &quot;#888888&quot;, lwd = 0.5, add = TRUE) plot(cmz, col = &quot;#888888&quot;, border = NA, add = TRUE) # modified plasma palette plasma_rev &lt;- rev(plasma(25, end = 0.9)) gray_int &lt;- colorRampPalette(c(&quot;#dddddd&quot;, plasma_rev[1])) pal &lt;- c(gray_int(4)[2], plasma_rev) # abundance vs. se if (nm == &quot;abd&quot;) { title &lt;- &quot;Australian Ibis Relative Abundance&quot; # set very low values to zero r_plot[r_plot &lt;= zero_threshold] &lt;- NA # log transform r_plot &lt;- log10(r_plot) # breaks and legend mx &lt;- cellStats(r_plot, max) mn &lt;- cellStats(r_plot, min) brks &lt;- seq(mn, mx, length.out = length(pal) + 1) lbl_brks &lt;- c(mn, (mn + mx) / 2, mx) lbls &lt;- round(10^lbl_brks, 3) } else { title &lt;- &quot;Australian Ibis Abundance Uncertainty (SE)&quot; # breaks and legend mx &lt;- ceiling(1000 * cellStats(r_plot, max)) / 1000 mn &lt;- floor(1000 * cellStats(r_plot, min)) / 1000 brks &lt;- seq(mn, mx, length.out = length(pal) + 1) lbl_brks &lt;- seq(mn, mx, length.out = 5) lbls &lt;- round(lbl_brks, 2) } # abundance plot(r_plot, col = pal, breaks = brks, maxpixels = ncell(r_plot), legend = FALSE, add = TRUE) # borders plot(ne_state_lines, col = &quot;#ffffff&quot;, lwd = 0.75, add = TRUE) box() # legend par(new = TRUE, mar = c(0, 0, 0, 0)) image.plot(zlim = range(brks), legend.only = TRUE, col = pal, smallplot = c(0.89, 0.92, 0.25, 0.75), horizontal = FALSE, axis.args = list(at = lbl_brks, labels = lbls, fg = &quot;black&quot;, col.axis = &quot;black&quot;, cex.axis = 0.75, lwd.ticks = 0.5, padj = 0), legend.args = list(text = title, side = 2, col = &quot;black&quot;, cex = 1, line = 0)) } 13.6 Exercises Now that you’ve completed this lesson, try modifying your script to complete at least one of the following exercises: Refit the model without effort variables and see how model performance changes. Predict from the same model for checklists of 10 minutes duration, instead of 1 hour. Compare the results and consider how the interpretation changes. Change the degrees of freedom for the covariate smooths and compare the fitted relationships. Refit the model with a random forest. Compare the predictions and the fitted relationships with covariates. Compare the encounter rate map to the relative abundance map. Fit an encounter rate random forest model then use the predicted encounter rate as a new covariate in the abundance model. Compare the model performance. Try producing a map of relative abundance using one of the other species in the example data. "]
]
